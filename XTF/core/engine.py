#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€åŒæ­¥å¼•æ“æ¨¡å—
æä¾›å¤šç»´è¡¨æ ¼å’Œç”µå­è¡¨æ ¼çš„ç»Ÿä¸€åŒæ­¥å¼•æ“
"""

import pandas as pd
import time
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any, List, Union, Tuple

from .config import SyncConfig, SyncMode, TargetType
from .converter import DataConverter
from api import FeishuAuth, RetryableAPIClient, BitableAPI, SheetAPI, RateLimiter


class XTFSyncEngine:
    """ç»Ÿä¸€åŒæ­¥å¼•æ“ - æ”¯æŒå¤šç»´è¡¨æ ¼å’Œç”µå­è¡¨æ ¼"""
    
    def __init__(self, config: SyncConfig):
        """
        åˆå§‹åŒ–åŒæ­¥å¼•æ“
        
        Args:
            config: ç»Ÿä¸€åŒæ­¥é…ç½®å¯¹è±¡
        """
        self.config = config
        
        # è®¾ç½®æ—¥å¿—ï¼ˆå¿…é¡»å…ˆè®¾ç½®ï¼Œå› ä¸ºå…¶ä»–åˆå§‹åŒ–å¯èƒ½éœ€è¦æ—¥å¿—ï¼‰
        self.setup_logging()
        self.logger = logging.getLogger('XTF.engine')
        
        # åˆå§‹åŒ–å…¨å±€è¯·æ±‚æ§åˆ¶å™¨ï¼ˆå¦‚æœé…ç½®äº†é«˜çº§é‡è¯•å’Œé¢‘æ§ç­–ç•¥ï¼‰
        self._init_global_controller()
        
        # åˆå§‹åŒ–APIç»„ä»¶
        self.auth = FeishuAuth(config.app_id, config.app_secret)
        self.api_client = RetryableAPIClient(
            max_retries=config.max_retries,
            rate_limiter=RateLimiter(config.rate_limit_delay)
        )
        
        # æ ¹æ®ç›®æ ‡ç±»å‹é€‰æ‹©APIå®¢æˆ·ç«¯
        if config.target_type == TargetType.BITABLE:
            self.api: Union[BitableAPI, SheetAPI] = BitableAPI(self.auth, self.api_client)
        else:  # SHEET
            self.api: Union[BitableAPI, SheetAPI] = SheetAPI(
                self.auth,
                self.api_client,
                start_row=self.config.start_row,
                start_column=self.config.start_column
            )
        # åˆå§‹åŒ–æ•°æ®è½¬æ¢å™¨
        self.converter = DataConverter(config.target_type)
    
    def _init_global_controller(self):
        """åˆå§‹åŒ–å…¨å±€è¯·æ±‚æ§åˆ¶å™¨"""
        try:
            from .config import ConfigManager
            # ä½¿ç”¨é…ç½®ç®¡ç†å™¨åˆ›å»ºå…¨å±€æ§åˆ¶å™¨
            global_controller = ConfigManager.create_request_controller(self.config)
            if global_controller:
                self.logger.info(f"å·²åˆå§‹åŒ–å…¨å±€è¯·æ±‚æ§åˆ¶å™¨ - é‡è¯•ç­–ç•¥: {self.config.retry_strategy_type}, "
                               f"é¢‘æ§ç­–ç•¥: {self.config.rate_limit_strategy_type}")
            else:
                self.logger.info(f"ä½¿ç”¨ä¼ ç»Ÿæ§åˆ¶æ¨¡å¼ - é‡è¯•æ¬¡æ•°: {self.config.max_retries}, "
                               f"é¢‘æ§é—´éš”: {self.config.rate_limit_delay}s")
        except Exception as e:
            self.logger.warning(f"åˆå§‹åŒ–å…¨å±€æ§åˆ¶å™¨å¤±è´¥ï¼Œå›é€€åˆ°ä¼ ç»Ÿæ¨¡å¼: {e}")
    
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_dir = Path('logs')
        log_dir.mkdir(exist_ok=True)
        
        target_name = "bitable" if self.config.target_type == TargetType.BITABLE else "sheet"
        log_file = log_dir / f"xtf_{target_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # è·å–XTFä¸“ç”¨çš„loggerï¼Œé¿å…å…¨å±€æ±¡æŸ“
        xtf_logger = logging.getLogger('XTF')
        xtf_logger.handlers.clear()
        
        level = getattr(logging, self.config.log_level.upper(), logging.INFO)
        xtf_logger.setLevel(level)
        
        # è®¾ç½®æ ¼å¼
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        xtf_logger.addHandler(file_handler)
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler(sys.stdout)
        console_handler.setFormatter(formatter)
        xtf_logger.addHandler(console_handler)
        
        # é˜²æ­¢ä¼ æ’­åˆ°æ ¹logger
        xtf_logger.propagate = False
    
    # ========== å¤šç»´è¡¨æ ¼ä¸“ç”¨æ–¹æ³• ==========
    
    def get_field_types(self) -> Dict[str, int]:
        """è·å–å¤šç»´è¡¨æ ¼å­—æ®µç±»å‹æ˜ å°„"""
        if self.config.target_type != TargetType.BITABLE:
            return {}
            
        try:
            if not isinstance(self.api, BitableAPI):
                return {}
            if not self.config.app_token or not self.config.table_id:
                self.logger.error("å¤šç»´è¡¨æ ¼çš„ app_token æˆ– table_id æœªé…ç½®")
                return {}
            existing_fields = self.api.list_fields(self.config.app_token, self.config.table_id)
            field_types = {}
            for field in existing_fields:
                field_name = field.get('field_name', '')
                field_type = field.get('type', 1)  # é»˜è®¤ä¸ºæ–‡æœ¬ç±»å‹
                field_types[field_name] = field_type
            
            self.logger.debug(f"è·å–åˆ° {len(field_types)} ä¸ªå­—æ®µç±»å‹ä¿¡æ¯")
            return field_types
            
        except Exception as e:
            self.logger.warning(f"è·å–å­—æ®µç±»å‹å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ™ºèƒ½ç±»å‹æ£€æµ‹")
            return {}

    def ensure_fields_exist(self, df: pd.DataFrame) -> Tuple[bool, Dict[str, int]]:
        """ç¡®ä¿å¤šç»´è¡¨æ ¼æ‰€éœ€å­—æ®µå­˜åœ¨"""
        if self.config.target_type != TargetType.BITABLE:
            return True, {}
            
        try:
            if not isinstance(self.api, BitableAPI):
                return False, {}
            if not self.config.app_token or not self.config.table_id:
                self.logger.error("å¤šç»´è¡¨æ ¼çš„ app_token æˆ– table_id æœªé…ç½®")
                return False, {}

            # è·å–ç°æœ‰å­—æ®µ
            existing_fields = self.api.list_fields(self.config.app_token, self.config.table_id)
            existing_field_names = {field['field_name'] for field in existing_fields}
            
            # æ„å»ºå­—æ®µç±»å‹æ˜ å°„
            field_types = {}
            for field in existing_fields:
                field_name = field.get('field_name', '')
                field_type = field.get('type', 1)
                field_types[field_name] = field_type
            
            if self.config.create_missing_fields:
                # æ‰¾å‡ºç¼ºå¤±çš„å­—æ®µï¼Œä¿æŒåŸå§‹åˆ—é¡ºåº
                required_fields = set(df.columns)
                missing_fields_set = required_fields - existing_field_names
                
                # æŒ‰ç…§ DataFrame åˆ—çš„åŸå§‹é¡ºåºæ’åˆ—ç¼ºå¤±å­—æ®µ
                missing_fields = [col for col in df.columns if col in missing_fields_set]
                
                if missing_fields:
                    self.logger.info(f"æ£€æµ‹åˆ° {len(missing_fields)} ä¸ªç¼ºå¤±å­—æ®µ")
                    self.logger.info(f"ä½¿ç”¨å­—æ®µç±»å‹ç­–ç•¥: {self.config.field_type_strategy.value}")
                    
                    # åˆ†ææ¯ä¸ªç¼ºå¤±å­—æ®µ
                    creation_plan = []
                    for field_name in missing_fields:
                        # ä½¿ç”¨å¢å¼ºçš„åˆ†ææ–¹æ³•
                        analysis = self.converter.analyze_excel_column_data_enhanced(
                            df, field_name, self.config.field_type_strategy.value, self.config
                        )
                        
                        creation_plan.append({
                            'field_name': field_name,
                            'suggested_type': analysis['suggested_feishu_type'],
                            'confidence': analysis['confidence'],
                            'reason': analysis['recommendation_reason'],
                            'has_validation': analysis['has_excel_validation']
                        })
                    
                    # æ˜¾ç¤ºåˆ›å»ºè®¡åˆ’
                    self.logger.info("=" * 60)
                    self.logger.info("ğŸ“‹ å­—æ®µåˆ›å»ºè®¡åˆ’:")
                    for plan in creation_plan:
                        validation_mark = "ğŸ“‹" if plan['has_validation'] else "ğŸ“"
                        self.logger.info(
                            f"{validation_mark} {plan['field_name']}: "
                            f"{self.converter.get_field_type_name(plan['suggested_type'])} "
                            f"(ç½®ä¿¡åº¦: {plan['confidence']:.1%}) - {plan['reason']}"
                        )
                    self.logger.info("=" * 60)
                    
                    # æ‰§è¡Œå­—æ®µåˆ›å»º
                    for plan in creation_plan:
                        if not isinstance(self.api, BitableAPI): continue
                        success = self.api.create_field(
                            self.config.app_token,
                            self.config.table_id,
                            plan['field_name'],
                            plan['suggested_type']
                        )
                        
                        if not success:
                            self.logger.error(f"å­—æ®µ '{plan['field_name']}' åˆ›å»ºå¤±è´¥")
                            return False, field_types
                        
                        # è®°å½•æ–°å­—æ®µç±»å‹
                        field_types[plan['field_name']] = plan['suggested_type']
                    
                    # ç­‰å¾…å­—æ®µåˆ›å»ºå®Œæˆ
                    import time
                    time.sleep(2)
                    
                else:
                    self.logger.info("âœ… æ‰€æœ‰å¿…éœ€å­—æ®µå·²å­˜åœ¨ï¼Œæ— éœ€åˆ›å»º")
            
            return True, field_types
            
        except Exception as e:
            self.logger.error(f"å­—æ®µæ£€æŸ¥å¤±è´¥: {e}")
            return False, {}
    
    def get_all_bitable_records(self) -> List[Dict]:
        """è·å–æ‰€æœ‰å¤šç»´è¡¨æ ¼è®°å½•"""
        if not isinstance(self.api, BitableAPI):
            return []
        if not self.config.app_token or not self.config.table_id:
            self.logger.error("å¤šç»´è¡¨æ ¼çš„ app_token æˆ– table_id æœªé…ç½®")
            return []
        return self.api.get_all_records(self.config.app_token, self.config.table_id)
    
    def process_in_batches(self, items: List[Any], batch_size: int,
                          processor_func, *args, **kwargs) -> bool:
        """åˆ†æ‰¹å¤„ç†æ•°æ®ï¼ˆå¤šç»´è¡¨æ ¼æ¨¡å¼ï¼‰"""
        if self.config.target_type != TargetType.BITABLE:
            return False
            
        total_batches = (len(items) + batch_size - 1) // batch_size
        success_count = 0
        
        # è·å–æ“ä½œç±»å‹ç”¨äºæ—¥å¿—æ˜¾ç¤º
        operation_type = self._get_operation_type(processor_func)
        
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            batch_num = i // batch_size + 1
            start_row = i + 1  # Excelè¡Œå·ä»1å¼€å§‹
            end_row = min(i + len(batch), len(items))
            
            try:
                # ä¿®å¤å‚æ•°ä¼ é€’é¡ºåºï¼šå…ˆä¼ é€’å›ºå®šå‚æ•°ï¼Œå†ä¼ é€’æ‰¹æ¬¡æ•°æ®
                if processor_func(*args, batch, **kwargs):
                    success_count += 1
                    # æ˜¾ç¤ºå…·ä½“çš„è¡ŒèŒƒå›´ä¿¡æ¯
                    range_info = f"ç¬¬{start_row}-{end_row}è¡Œ" if start_row != end_row else f"ç¬¬{start_row}è¡Œ"
                    self.logger.info(f"âœ… {operation_type}æˆåŠŸ: æ‰¹æ¬¡{batch_num}/{total_batches}, {len(batch)}æ¡è®°å½• ({range_info})")
                else:
                    self.logger.error(f"âŒ {operation_type}å¤±è´¥: æ‰¹æ¬¡{batch_num}/{total_batches}")
            except Exception as e:
                self.logger.error(f"âŒ {operation_type}å¼‚å¸¸: æ‰¹æ¬¡{batch_num}/{total_batches}, é”™è¯¯: {e}")
        
        self.logger.info(f"ğŸ‰ {operation_type}å®Œæˆ: {success_count}/{total_batches} ä¸ªæ‰¹æ¬¡æˆåŠŸ")
        return success_count == total_batches
    
    def _get_operation_type(self, processor_func) -> str:
        """æ ¹æ®å¤„ç†å‡½æ•°è·å–æ“ä½œç±»å‹"""
        func_name = getattr(processor_func, '__name__', str(processor_func))
        if 'create' in func_name:
            return "æ‰¹é‡åˆ›å»º"
        elif 'update' in func_name:
            return "æ‰¹é‡æ›´æ–°"
        elif 'delete' in func_name:
            return "æ‰¹é‡åˆ é™¤"
        else:
            return "æ‰¹é‡å¤„ç†"
    
    # ========== ç”µå­è¡¨æ ¼ä¸“ç”¨æ–¹æ³• ==========
    
    def get_current_sheet_data(self) -> pd.DataFrame:
        """è·å–å½“å‰ç”µå­è¡¨æ ¼æ•°æ®"""
        if self.config.target_type != TargetType.SHEET:
            return pd.DataFrame()
            
        # æ„å»ºä»é…ç½®èµ·å§‹ç‚¹å¼€å§‹çš„è¯»å–èŒƒå›´
        start_cell = f"{self.config.start_column}{self.config.start_row}"
        # å®šä¹‰ä¸€ä¸ªè¶³å¤Ÿå¤§çš„èŒƒå›´æ¥æ•è·æ‰€æœ‰æ•°æ®ï¼Œé£ä¹¦APIä¼šè‡ªåŠ¨è£å‰ªåˆ°æœ‰æ•°æ®çš„å®é™…èŒƒå›´
        read_range = f"{self.config.sheet_id}!{start_cell}:ZZ500000"
        
        self.logger.info(f"å°è¯•ä»èŒƒå›´è¯»å–æ•°æ®: {read_range}")

        try:
            if not isinstance(self.api, SheetAPI): return pd.DataFrame()
            if not self.config.spreadsheet_token:
                self.logger.error("ç”µå­è¡¨æ ¼çš„ spreadsheet_token æœªé…ç½®")
                return pd.DataFrame()
                
            values = self.api.get_sheet_data(self.config.spreadsheet_token, read_range)
            df = self.converter.values_to_df(values)
            
            if not df.empty:
                # æ£€æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆæ•°æ®ï¼ˆè‡³å°‘æœ‰ä¸€è¡Œæ•°æ®åŒ…å«éç©ºå€¼ï¼‰
                has_valid_data = False
                for _, row in df.iterrows():
                    if any(pd.notnull(val) and str(val).strip() != '' for val in row):
                        has_valid_data = True
                        break
                
                if has_valid_data:
                    self.logger.info(f"æˆåŠŸè·å–ç”µå­è¡¨æ ¼æ•°æ®: {len(df)} è¡Œ x {len(df.columns)} åˆ— (ä» {start_cell} å¼€å§‹)")
                    return df
            
            # å¦‚æœdfä¸ºç©ºæˆ–æ•°æ®å…¨ä¸ºç©ºï¼Œè¯´æ˜è¡¨æ ¼åœ¨æŒ‡å®šèŒƒå›´ç¡®å®æ˜¯ç©ºçš„
            self.logger.info(f"åœ¨èŒƒå›´ {read_range} å†…æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè§†ä¸ºç©ºè¡¨")
            return pd.DataFrame()

        except Exception as e:
            self.logger.warning(f"å°è¯•ä»èŒƒå›´ {read_range} è¯»å–æ•°æ®å¤±è´¥: {e}")
            self.logger.warning("æ— æ³•è·å–ç”µå­è¡¨æ ¼æ•°æ®ï¼Œå°†ä½¿ç”¨è¦†ç›–æ¨¡å¼")
            return pd.DataFrame()
    
    # ========== é€‰æ‹©æ€§åŒæ­¥è¾…åŠ©æ–¹æ³• ==========
    
    def _apply_selective_filter(self, df: pd.DataFrame) -> pd.DataFrame:
        """åº”ç”¨é€‰æ‹©æ€§åˆ—è¿‡æ»¤"""
        if not self.config.selective_sync.enabled or not self.config.selective_sync.columns:
            return df
        
        # è·å–è¦å¤„ç†çš„åˆ—
        target_columns = self.config.selective_sync.columns.copy()
        
        # è‡ªåŠ¨åŒ…å«ç´¢å¼•åˆ—ï¼ˆç”¨äºåŒ¹é…é€»è¾‘ï¼‰
        if (self.config.selective_sync.auto_include_index and 
            self.config.index_column and 
            self.config.index_column not in target_columns):
            target_columns.append(self.config.index_column)
            self.logger.info(f"è‡ªåŠ¨åŒ…å«ç´¢å¼•åˆ—: {self.config.index_column}")
        
        # éªŒè¯åˆ—æ˜¯å¦å­˜åœ¨
        missing_columns = [col for col in target_columns if col not in df.columns]
        if missing_columns:
            self.logger.warning(f"æŒ‡å®šçš„åˆ—ä¸å­˜åœ¨äºæ•°æ®ä¸­: {missing_columns}")
            target_columns = [col for col in target_columns if col in df.columns]
        
        # ä¿æŒåˆ—é¡ºåºï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.selective_sync.preserve_column_order:
            # æŒ‰åŸå§‹DataFrameä¸­çš„é¡ºåºæ’åˆ—
            ordered_columns = [col for col in df.columns if col in target_columns]
            return df[ordered_columns]
        else:
            return df[target_columns]
    
    # ========== ç»Ÿä¸€åŒæ­¥æ–¹æ³• ==========
    
    def sync_full(self, df: pd.DataFrame) -> bool:
        """å…¨é‡åŒæ­¥ï¼šå·²å­˜åœ¨çš„æ›´æ–°ï¼Œä¸å­˜åœ¨çš„æ–°å¢"""
        self.logger.info("å¼€å§‹å…¨é‡åŒæ­¥...")
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨é€‰æ‹©æ€§åŒæ­¥
        if self.config.selective_sync.enabled:
            df = self._apply_selective_filter(df)
            self.logger.info(f"é€‰æ‹©æ€§åŒæ­¥å·²å¯ç”¨ï¼Œå¤„ç† {len(self.config.selective_sync.columns) if self.config.selective_sync.columns else 'æ‰€æœ‰'} åˆ—")
        
        if self.config.target_type == TargetType.BITABLE:
            return self._sync_full_bitable(df)
        else:  # SHEET
            return self._sync_full_sheet(df)
    
    def _sync_full_bitable(self, df: pd.DataFrame) -> bool:
        """å¤šç»´è¡¨æ ¼å…¨é‡åŒæ­¥"""
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ‰§è¡Œçº¯æ–°å¢æ“ä½œ")
            field_types = self.get_field_types()
            new_records = self.converter.df_to_records(df, field_types)
            if isinstance(self.api, BitableAPI) and self.config.app_token and self.config.table_id:
                return self.process_in_batches(
                    new_records, self.config.batch_size,
                    self.api.batch_create_records,
                    self.config.app_token, self.config.table_id
                )
            return False
        
        # è·å–ç°æœ‰è®°å½•å¹¶å»ºç«‹ç´¢å¼•
        existing_records = self.get_all_bitable_records()
        self.logger.info(f"ğŸ” è·å–åˆ°ç°æœ‰è®°å½•æ•°é‡: {len(existing_records)}")
        
        existing_index = self.converter.build_record_index(existing_records, self.config.index_column)
        self.logger.info(f"ğŸ” æ„å»ºç´¢å¼•æˆåŠŸï¼Œç´¢å¼•æ•°é‡: {len(existing_index)}")
        
        # æ‰“å°å‰å‡ ä¸ªç°æœ‰è®°å½•çš„ç´¢å¼•åˆ—å€¼ç”¨äºè°ƒè¯•
        if existing_records and len(existing_records) > 0:
            for i, record in enumerate(existing_records[:3]):
                fields = record.get('fields', {})
                index_value = fields.get(self.config.index_column, 'æœªæ‰¾åˆ°')
                self.logger.info(f"ğŸ” ç°æœ‰è®°å½• {i+1} ç´¢å¼•åˆ— '{self.config.index_column}' å€¼: '{index_value}'")
        
        field_types = self.get_field_types()
        
        # åˆ†ç±»æœ¬åœ°æ•°æ®
        records_to_update = []
        records_to_create = []
        
        for i, (_, row) in enumerate(df.iterrows()):
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            index_value = row.get(self.config.index_column, 'æœªæ‰¾åˆ°')
            
            # æ‰“å°å‰å‡ æ¡è®°å½•çš„åŒ¹é…ä¿¡æ¯ç”¨äºè°ƒè¯•
            if i < 3:
                self.logger.info(f"ğŸ” æ–°æ•°æ®è®°å½• {i+1} ç´¢å¼•åˆ— '{self.config.index_column}' å€¼: '{index_value}' -> å“ˆå¸Œ: {index_hash}")
                self.logger.info(f"ğŸ” å“ˆå¸Œæ˜¯å¦åœ¨ç°æœ‰ç´¢å¼•ä¸­: {index_hash in existing_index if index_hash else False}")
            
            # ä½¿ç”¨å­—æ®µç±»å‹è½¬æ¢æ„å»ºè®°å½•
            fields = {}
            for k, v in row.to_dict().items():
                if pd.notnull(v):
                    converted_value = self.converter.convert_field_value_safe(str(k), v, field_types)
                    if converted_value is not None:
                        fields[str(k)] = converted_value
            
            record = {"fields": fields}
            
            if index_hash and index_hash in existing_index:
                # éœ€è¦æ›´æ–°çš„è®°å½•
                existing_record = existing_index[index_hash]
                record["record_id"] = existing_record["record_id"]
                records_to_update.append(record)
            else:
                # éœ€è¦æ–°å¢çš„è®°å½•
                records_to_create.append(record)
        
        self.logger.info(f"å…¨é‡åŒæ­¥è®¡åˆ’: æ›´æ–° {len(records_to_update)} æ¡ï¼Œæ–°å¢ {len(records_to_create)} æ¡")
        
        # æ‰§è¡Œæ›´æ–°
        update_success = True
        if records_to_update and isinstance(self.api, BitableAPI) and self.config.app_token and self.config.table_id:
            update_success = self.process_in_batches(
                records_to_update, self.config.batch_size,
                self.api.batch_update_records,
                self.config.app_token, self.config.table_id
            )
        
        # æ‰§è¡Œæ–°å¢
        create_success = True
        if records_to_create and isinstance(self.api, BitableAPI) and self.config.app_token and self.config.table_id:
            create_success = self.process_in_batches(
                records_to_create, self.config.batch_size,
                self.api.batch_create_records,
                self.config.app_token, self.config.table_id
            )
        
        return update_success and create_success
    
    def _sync_full_sheet(self, df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼å…¨é‡åŒæ­¥"""
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ‰§è¡Œå®Œå…¨è¦†ç›–æ“ä½œ")
            return self.sync_clone(df)
        
        # è·å–ç°æœ‰æ•°æ®
        current_df = self.get_current_sheet_data()
        
        if current_df.empty:
            self.logger.info("ç”µå­è¡¨æ ¼ä¸ºç©ºï¼Œæ‰§è¡Œæ–°å¢æ“ä½œ")
            return self.sync_clone(df)
        
        # â­ å…³é”®ä¿®æ”¹ï¼šæ£€æŸ¥æ˜¯å¦å¯ç”¨é€‰æ‹©æ€§åŒæ­¥ï¼Œä½¿ç”¨ç²¾ç¡®åˆ—çº§æ§åˆ¶
        if self.config.selective_sync.enabled and self.config.selective_sync.columns:
            self.logger.info(f"ğŸ¯ å¯ç”¨ç²¾ç¡®åˆ—çº§æ§åˆ¶åŒæ­¥: {self.config.selective_sync.columns}")
            return self._sync_selective_columns_sheet(df, current_df)
        
        # åŸæœ‰çš„å®Œæ•´è¡¨æ ¼åŒæ­¥é€»è¾‘
        current_index = self.converter.build_data_index(current_df, self.config.index_column)
        
        # åˆ†ç±»æ•°æ®
        update_rows = []
        new_rows = []
        
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            if index_hash and index_hash in current_index:
                # æ›´æ–°ç°æœ‰è¡Œ
                current_row_idx = current_index[index_hash]
                update_rows.append((current_row_idx, row))
            else:
                # æ–°å¢è¡Œ
                new_rows.append(row)
        
        self.logger.info(f"å…¨é‡åŒæ­¥è®¡åˆ’: æ›´æ–° {len(update_rows)} è¡Œï¼Œæ–°å¢ {len(new_rows)} è¡Œ")
        
        # æ‰§è¡Œæ›´æ–°
        success = True
        if update_rows:
            # æ›´æ–°ç°æœ‰è¡Œ
            updated_df = current_df.copy()
            for current_row_idx, new_row in update_rows:
                for col in df.columns:
                    if col in updated_df.columns:
                        updated_df.iloc[current_row_idx][col] = new_row[col]
            
            # å†™å…¥æ›´æ–°åçš„æ•°æ®
            values = self.converter.df_to_values(updated_df)
            if isinstance(self.api, SheetAPI) and self.config.spreadsheet_token and self.config.sheet_id:
                success = self.api.write_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    values,
                    self.config.batch_size,
                    80,  # åˆ—æ‰¹æ¬¡å¤§å°ï¼Œä¿æŒå®‰å…¨è£•åº¦
                    self.config.rate_limit_delay
                )
        
        # è¿½åŠ æ–°è¡Œ
        if new_rows and success:
            new_df = pd.DataFrame(new_rows)
            new_values = self.converter.df_to_values(new_df, include_headers=False)
            
            if new_values and isinstance(self.api, SheetAPI) and self.config.spreadsheet_token and self.config.sheet_id:
                self.logger.info(f"å¼€å§‹è¿½åŠ  {len(new_values)} è¡Œæ–°æ•°æ®")
                success = self.api.append_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    new_values,
                    self.config.batch_size,
                    self.config.rate_limit_delay
                )
        
        return success

    def _sync_selective_columns_sheet(self, df: pd.DataFrame, current_df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼é€‰æ‹©æ€§åˆ—åŒæ­¥ - ä½¿ç”¨ç²¾ç¡®åˆ—æ§åˆ¶"""
        self.logger.info(f"ğŸ¯ å¯ç”¨ç²¾ç¡®åˆ—æ§åˆ¶åŒæ­¥: {self.config.selective_sync.columns}")
        
        # æ„å»ºç´¢å¼•
        current_index = self.converter.build_data_index(current_df, self.config.index_column)
        
        # å‡†å¤‡æ›´æ–°æ•°æ®æ˜ å°„ {row_idx: {col: value}}
        update_data_map = {}
        new_rows = []
        
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            if index_hash and index_hash in current_index:
                # æ›´æ–°ç°æœ‰è¡Œ
                current_row_idx = current_index[index_hash]
                if current_row_idx not in update_data_map:
                    update_data_map[current_row_idx] = {}
                
                # åªæ›´æ–°æŒ‡å®šåˆ—
                for col in self.config.selective_sync.columns:
                    if col in df.columns:
                        update_data_map[current_row_idx][col] = row[col]
            else:
                # æ–°å¢è¡Œ
                new_rows.append(row)
        
        self.logger.info(f"ç²¾ç¡®åˆ—æ§åˆ¶è®¡åˆ’: æ›´æ–° {len(update_data_map)} è¡Œçš„æŒ‡å®šåˆ—ï¼Œæ–°å¢ {len(new_rows)} è¡Œ")
        
        success = True
        
        # æ‰§è¡Œé€‰æ‹©æ€§åˆ—æ›´æ–°
        if update_data_map:
            success = self._update_selective_columns(current_df, update_data_map)
        
        # è¿½åŠ æ–°è¡Œï¼ˆå¦‚æœæœ‰ï¼‰
        if new_rows and success:
            new_df = pd.DataFrame(new_rows)
            new_values = self.converter.df_to_values(new_df, include_headers=False)
            
            if isinstance(self.api, SheetAPI) and self.config.spreadsheet_token and self.config.sheet_id:
                self.logger.info(f"å¼€å§‹è¿½åŠ  {len(new_values)} è¡Œæ–°æ•°æ®")
                success = self.api.append_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    new_values,
                    self.config.batch_size,
                    self.config.rate_limit_delay
                )
        
        return success

    def _update_selective_columns(self, current_df: pd.DataFrame, update_data_map: Dict[int, Dict[str, Any]]) -> bool:
        """ä½¿ç”¨ç²¾ç¡®åˆ—æ§åˆ¶æ›´æ–°æ•°æ®"""
        if not update_data_map:
            return True
        
        # å‡†å¤‡æŒ‰åˆ—ç»„ç»‡çš„æ›´æ–°æ•°æ®
        columns_to_update = set()
        for row_updates in update_data_map.values():
            columns_to_update.update(row_updates.keys())
        
        self.logger.info(f"ğŸ”„ å‡†å¤‡æ›´æ–°åˆ—: {list(columns_to_update)}")
        
        # ä½¿ç”¨ converter çš„ df_to_column_data å’Œ get_column_positions
        # æ„å»ºéœ€è¦æ›´æ–°çš„åˆ—æ•°æ®
        column_data = {}
        for col in columns_to_update:
            col_data = []
            for row_idx in range(len(current_df)):
                if row_idx in update_data_map and col in update_data_map[row_idx]:
                    # ä½¿ç”¨æ–°å€¼
                    converted_value = self.converter.simple_convert_value(update_data_map[row_idx][col])
                    col_data.append(converted_value)
                else:
                    # ä¿æŒåŸå€¼
                    original_value = current_df.iloc[row_idx][col] if col in current_df.columns else ""
                    converted_value = self.converter.simple_convert_value(original_value)
                    col_data.append(converted_value)
            column_data[col] = col_data
        
        # è·å–åˆ—ä½ç½®æ˜ å°„
        column_positions = self.converter.get_column_positions(current_df, list(columns_to_update))
        
        self.logger.info(f"ğŸ“ åˆ—ä½ç½®æ˜ å°„: {column_positions}")
        
        # ä½¿ç”¨ç²¾ç¡®åˆ—å†™å…¥API
        if isinstance(self.api, SheetAPI) and self.config.spreadsheet_token and self.config.sheet_id:
            return self.api.write_selective_columns(
                self.config.spreadsheet_token,
                self.config.sheet_id,
                column_data,
                column_positions,
                start_row=2,  # å‡è®¾ç¬¬1è¡Œæ˜¯è¡¨å¤´
                rate_limit_delay=self.config.rate_limit_delay
            )
        
        return False
    
    def sync_incremental(self, df: pd.DataFrame) -> bool:
        """å¢é‡åŒæ­¥ï¼šåªæ–°å¢ä¸å­˜åœ¨çš„è®°å½•"""
        self.logger.info("å¼€å§‹å¢é‡åŒæ­¥...")
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨é€‰æ‹©æ€§åŒæ­¥
        if self.config.selective_sync.enabled:
            df = self._apply_selective_filter(df)
            self.logger.info(f"é€‰æ‹©æ€§åŒæ­¥å·²å¯ç”¨ï¼Œå¤„ç† {len(self.config.selective_sync.columns) if self.config.selective_sync.columns else 'æ‰€æœ‰'} åˆ—")
        
        if self.config.target_type == TargetType.BITABLE:
            return self._sync_incremental_bitable(df)
        else:  # SHEET
            return self._sync_incremental_sheet(df)
    
    def _sync_incremental_bitable(self, df: pd.DataFrame) -> bool:
        """å¤šç»´è¡¨æ ¼å¢é‡åŒæ­¥"""
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ‰§è¡Œçº¯æ–°å¢æ“ä½œ")
            field_types = self.get_field_types()
            new_records = self.converter.df_to_records(df, field_types)
            if isinstance(self.api, BitableAPI) and self.config.app_token and self.config.table_id:
                return self.process_in_batches(
                    new_records, self.config.batch_size,
                    self.api.batch_create_records,
                    self.config.app_token, self.config.table_id
                )
            return False
        
        # è·å–ç°æœ‰è®°å½•å¹¶å»ºç«‹ç´¢å¼•
        existing_records = self.get_all_bitable_records()
        existing_index = self.converter.build_record_index(existing_records, self.config.index_column)
        field_types = self.get_field_types()
        
        # ç­›é€‰å‡ºéœ€è¦æ–°å¢çš„è®°å½•
        records_to_create = []
        
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            
            if not index_hash or index_hash not in existing_index:
                # ä½¿ç”¨å­—æ®µç±»å‹è½¬æ¢æ„å»ºè®°å½•
                fields = {}
                for k, v in row.to_dict().items():
                    if pd.notnull(v):
                        converted_value = self.converter.convert_field_value_safe(str(k), v, field_types)
                        if converted_value is not None:
                            fields[str(k)] = converted_value
                
                record = {"fields": fields}
                records_to_create.append(record)
        
        self.logger.info(f"å¢é‡åŒæ­¥è®¡åˆ’: æ–°å¢ {len(records_to_create)} æ¡è®°å½•")
        
        if records_to_create and isinstance(self.api, BitableAPI) and self.config.app_token and self.config.table_id:
            return self.process_in_batches(
                records_to_create, self.config.batch_size,
                self.api.batch_create_records,
                self.config.app_token, self.config.table_id
            )
        else:
            self.logger.info("æ²¡æœ‰æ–°è®°å½•éœ€è¦åŒæ­¥")
            return True
    
    def _sync_incremental_sheet(self, df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼å¢é‡åŒæ­¥ - ä½¿ç”¨ä¼˜åŒ–APIç­–ç•¥"""
        if not self.config.index_column:
            self.logger.warning("æœªæŒ‡å®šç´¢å¼•åˆ—ï¼Œå°†æ–°å¢å…¨éƒ¨æ•°æ®")
            
            # â­ æ£€æŸ¥é€‰æ‹©æ€§åŒæ­¥ï¼šå¦‚æœå¯ç”¨ï¼Œéœ€è¦ç”¨åˆ—çº§æ§åˆ¶è¿½åŠ 
            if self.config.selective_sync.enabled and self.config.selective_sync.columns:
                self.logger.info(f"ğŸ¯ å¢é‡åŒæ­¥å¯ç”¨ç²¾ç¡®åˆ—æ§åˆ¶: {self.config.selective_sync.columns}")
                return self._append_selective_columns(df)
            
            # å¸¸è§„å¢é‡åŒæ­¥ç­–ç•¥
            values = self.converter.df_to_values(df, include_headers=False) # è¿½åŠ ä¸éœ€è¦è¡¨å¤´
            self.logger.info(f"ä½¿ç”¨appendæ¥å£è¿›è¡Œå¢é‡åŒæ­¥")
            if isinstance(self.api, SheetAPI) and self.config.spreadsheet_token and self.config.sheet_id:
                return self.api.append_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    values,
                    self.config.batch_size,
                    self.config.rate_limit_delay
                )
            return False
        
        # è·å–ç°æœ‰æ•°æ®
        current_df = self.get_current_sheet_data()
        
        if current_df.empty:
            self.logger.info("ç”µå­è¡¨æ ¼ä¸ºç©ºï¼Œæ–°å¢å…¨éƒ¨æ•°æ®")
            # â­ æ£€æŸ¥é€‰æ‹©æ€§åŒæ­¥
            if self.config.selective_sync.enabled and self.config.selective_sync.columns:
                return self._append_selective_columns(df)
            # ä½¿ç”¨å…‹éš†åŒæ­¥ï¼ˆä¼šå…ˆå†™å…¥æ•°æ®å†è®¾ç½®æ ¼å¼ï¼‰
            return self.sync_clone(df)
        
        # æ„å»ºç´¢å¼•
        current_index = self.converter.build_data_index(current_df, self.config.index_column)
        
        # ç­›é€‰éœ€è¦æ–°å¢çš„è®°å½•
        new_rows = []
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            if not index_hash or index_hash not in current_index:
                new_rows.append(row)
        
        self.logger.info(f"å¢é‡åŒæ­¥è®¡åˆ’: æ–°å¢ {len(new_rows)} è¡Œ")
        
        if new_rows:
            new_df = pd.DataFrame(new_rows)
            
            # â­ æ£€æŸ¥é€‰æ‹©æ€§åŒæ­¥ï¼šå¦‚æœå¯ç”¨ï¼Œéœ€è¦ç”¨åˆ—çº§æ§åˆ¶è¿½åŠ 
            if self.config.selective_sync.enabled and self.config.selective_sync.columns:
                return self._append_selective_columns(new_df)
            
            # å¸¸è§„è¿½åŠ 
            new_values = self.converter.df_to_values(new_df, include_headers=False)
            
            # è¿½åŠ æ–°æ•°æ®
            if isinstance(self.api, SheetAPI) and self.config.spreadsheet_token and self.config.sheet_id:
                self.logger.info(f"å¼€å§‹å¢é‡è¿½åŠ  {len(new_values)} è¡Œæ•°æ®")
                return self.api.append_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    new_values,
                    self.config.batch_size,
                    self.config.rate_limit_delay
                )
            return False
        else:
            self.logger.info("æ²¡æœ‰æ–°è®°å½•éœ€è¦åŒæ­¥")
            return True

    def _append_selective_columns(self, df: pd.DataFrame) -> bool:
        """é€‰æ‹©æ€§åˆ—çš„è¿½åŠ æ“ä½œ"""
        if not self.config.selective_sync.enabled or not self.config.selective_sync.columns:
            self.logger.warning("é€‰æ‹©æ€§åŒæ­¥æœªå¯ç”¨æˆ–æœªæŒ‡å®šåˆ—ï¼Œä½¿ç”¨å¸¸è§„è¿½åŠ ")
            values = self.converter.df_to_values(df, include_headers=False)
            if isinstance(self.api, SheetAPI) and self.config.spreadsheet_token and self.config.sheet_id:
                return self.api.append_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    values,
                    self.config.batch_size,
                    self.config.rate_limit_delay
                )
            return False
        
        # è·å–å½“å‰è¡¨æ ¼æ•°æ®ä»¥ç¡®å®šæ­£ç¡®çš„åˆ—ä½ç½®
        current_df = self.get_current_sheet_data()
        if current_df.empty:
            # å¦‚æœè¡¨æ ¼ä¸ºç©ºï¼Œå…ˆå†™å…¥è¡¨å¤´ï¼Œç„¶åè¿½åŠ æ•°æ®
            self.logger.info("è¡¨æ ¼ä¸ºç©ºï¼Œå…ˆåˆ›å»ºè¡¨å¤´ç„¶åè¿½åŠ é€‰æ‹©æ€§åˆ—æ•°æ®")
            headers = [col for col in df.columns if col in self.config.selective_sync.columns]
            header_values = [headers]
            
            # å†™å…¥è¡¨å¤´
            if isinstance(self.api, SheetAPI) and self.config.spreadsheet_token and self.config.sheet_id:
                header_success = self.api.write_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    header_values,
                    self.config.batch_size,
                    80,
                    self.config.rate_limit_delay
                )
                if not header_success:
                    return False
                
                # æ›´æ–°current_dfä¸ºåŒ…å«è¡¨å¤´çš„ç©ºæ•°æ®æ¡†
                current_df = pd.DataFrame(columns=headers)
        
        # å‡†å¤‡é€‰æ‹©æ€§åˆ—æ•°æ®
        column_data = self.converter.df_to_column_data(df, self.config.selective_sync.columns)
        column_positions = self.converter.get_column_positions(current_df, self.config.selective_sync.columns)
        
        # è®¡ç®—èµ·å§‹è¡Œï¼šå½“å‰æ•°æ®è¡Œæ•° + 1ï¼ˆè¡¨å¤´ï¼‰ + 1
        start_row = len(current_df) + 2
        
        self.logger.info(f"ğŸ¯ é€‰æ‹©æ€§åˆ—è¿½åŠ : {list(column_data.keys())} ä»ç¬¬{start_row}è¡Œå¼€å§‹")
        
        # ä½¿ç”¨ç²¾ç¡®åˆ—è¿½åŠ API
        if isinstance(self.api, SheetAPI) and self.config.spreadsheet_token and self.config.sheet_id:
            return self.api.write_selective_columns(
                self.config.spreadsheet_token,
                self.config.sheet_id,
                column_data,
                column_positions,
                start_row=start_row,
                rate_limit_delay=self.config.rate_limit_delay
            )
        
        return False
    
    def sync_overwrite(self, df: pd.DataFrame) -> bool:
        """è¦†ç›–åŒæ­¥ï¼šåˆ é™¤å·²å­˜åœ¨çš„ï¼Œç„¶åæ–°å¢å…¨éƒ¨"""
        self.logger.info("å¼€å§‹è¦†ç›–åŒæ­¥...")
        
        if not self.config.index_column:
            self.logger.error("è¦†ç›–åŒæ­¥æ¨¡å¼éœ€è¦æŒ‡å®šç´¢å¼•åˆ—")
            return False
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨é€‰æ‹©æ€§åŒæ­¥
        if self.config.selective_sync.enabled:
            df = self._apply_selective_filter(df)
            self.logger.info(f"é€‰æ‹©æ€§åŒæ­¥å·²å¯ç”¨ï¼Œå¤„ç† {len(self.config.selective_sync.columns) if self.config.selective_sync.columns else 'æ‰€æœ‰'} åˆ—")
        
        if self.config.target_type == TargetType.BITABLE:
            return self._sync_overwrite_bitable(df)
        else:  # SHEET
            return self._sync_overwrite_sheet(df)
    
    def _sync_overwrite_bitable(self, df: pd.DataFrame) -> bool:
        """å¤šç»´è¡¨æ ¼è¦†ç›–åŒæ­¥"""
        # è·å–ç°æœ‰è®°å½•å¹¶å»ºç«‹ç´¢å¼•
        existing_records = self.get_all_bitable_records()
        existing_index = self.converter.build_record_index(existing_records, self.config.index_column)
        field_types = self.get_field_types()
        
        # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„è®°å½•
        record_ids_to_delete = []
        
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            if index_hash and index_hash in existing_index:
                existing_record = existing_index[index_hash]
                record_ids_to_delete.append(existing_record["record_id"])
        
        self.logger.info(f"è¦†ç›–åŒæ­¥è®¡åˆ’: åˆ é™¤ {len(record_ids_to_delete)} æ¡å·²å­˜åœ¨è®°å½•ï¼Œç„¶åæ–°å¢ {len(df)} æ¡è®°å½•")
        
        # åˆ é™¤å·²å­˜åœ¨çš„è®°å½•
        delete_success = True
        if record_ids_to_delete and isinstance(self.api, BitableAPI) and self.config.app_token and self.config.table_id:
            delete_success = self.process_in_batches(
                record_ids_to_delete, self.config.batch_size,
                self.api.batch_delete_records,
                self.config.app_token, self.config.table_id
            )
        
        # æ–°å¢å…¨éƒ¨è®°å½•
        new_records = self.converter.df_to_records(df, field_types)
        create_success = False
        if isinstance(self.api, BitableAPI) and self.config.app_token and self.config.table_id:
            create_success = self.process_in_batches(
                new_records, self.config.batch_size,
                self.api.batch_create_records,
                self.config.app_token, self.config.table_id
            )
        
        return delete_success and create_success
    
    def _sync_overwrite_sheet(self, df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼è¦†ç›–åŒæ­¥"""
        # è·å–ç°æœ‰æ•°æ®
        current_df = self.get_current_sheet_data()
        
        if current_df.empty:
            self.logger.info("ç”µå­è¡¨æ ¼ä¸ºç©ºï¼Œæ‰§è¡Œæ–°å¢æ“ä½œ")
            return self.sync_clone(df)
        
        # â­ æ£€æŸ¥æ˜¯å¦å¯ç”¨é€‰æ‹©æ€§åŒæ­¥ï¼Œä½¿ç”¨ç²¾ç¡®åˆ—çº§æ§åˆ¶
        if self.config.selective_sync.enabled and self.config.selective_sync.columns:
            self.logger.info(f"ğŸ¯ è¦†ç›–åŒæ­¥å¯ç”¨ç²¾ç¡®åˆ—æ§åˆ¶: {self.config.selective_sync.columns}")
            return self._sync_overwrite_selective_columns_sheet(df, current_df)
        
        # åŸæœ‰çš„å®Œæ•´è¡¨æ ¼è¦†ç›–é€»è¾‘
        new_df_rows = []
        deleted_count = 0
        
        # ä¿ç•™ä¸åœ¨æ–°æ•°æ®ä¸­çš„ç°æœ‰è®°å½•
        for _, row in current_df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            if index_hash:
                # æ£€æŸ¥æ˜¯å¦åœ¨æ–°æ•°æ®ä¸­
                found_in_new = False
                for _, new_row in df.iterrows():
                    new_index_hash = self.converter.get_index_value_hash(new_row, self.config.index_column)
                    if new_index_hash == index_hash:
                        found_in_new = True
                        break
                
                if not found_in_new:
                    new_df_rows.append(row)
                else:
                    deleted_count += 1
        
        # æ·»åŠ æ–°æ•°æ®
        for _, row in df.iterrows():
            new_df_rows.append(row)
        
        self.logger.info(f"è¦†ç›–åŒæ­¥è®¡åˆ’: åˆ é™¤ {deleted_count} è¡Œï¼Œæ–°å¢ {len(df)} è¡Œ")
        
        # é‡å†™æ•´ä¸ªè¡¨æ ¼
        if new_df_rows:
            new_df = pd.DataFrame(new_df_rows)
            values = self.converter.df_to_values(new_df)
            
            # ä½¿ç”¨ä¼˜åŒ–APIç­–ç•¥è¦†ç›–å†™å…¥
            if isinstance(self.api, SheetAPI) and self.config.spreadsheet_token and self.config.sheet_id:
                self.logger.info(f"ä½¿ç”¨write_sheet_dataè¦†ç›–å†™å…¥")
                return self.api.write_sheet_data(
                    self.config.spreadsheet_token,
                    self.config.sheet_id,
                    values,
                    self.config.batch_size,
                    80, # col_batch_size
                    self.config.rate_limit_delay
                )
            return False
        else:
            # å¦‚æœæ²¡æœ‰æ•°æ®ï¼Œæ¸…ç©ºè¡¨æ ¼
            if isinstance(self.api, SheetAPI) and self.config.spreadsheet_token and self.config.sheet_id:
                # å‡è®¾ä¸€ä¸ªè¶³å¤Ÿå¤§çš„èŒƒå›´æ¥æ¸…ç©º
                return self.api.clear_sheet_data(self.config.spreadsheet_token, self.config.sheet_id, "A1:ZZZ10000")
            return False

    def _sync_overwrite_selective_columns_sheet(self, df: pd.DataFrame, current_df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼é€‰æ‹©æ€§åˆ—è¦†ç›–åŒæ­¥"""
        self.logger.info(f"ğŸ¯ é€‰æ‹©æ€§åˆ—è¦†ç›–åŒæ­¥: {self.config.selective_sync.columns}")
        
        # æ„å»ºç´¢å¼•
        current_index = self.converter.build_data_index(current_df, self.config.index_column)
        
        # å‡†å¤‡æ•°æ®æ˜ å°„ {row_idx: {col: value}}
        update_data_map = {}  # æ›´æ–°ç°æœ‰è¡Œçš„æŒ‡å®šåˆ—
        new_rows = []         # å…¨æ–°çš„è¡Œ
        
        for _, row in df.iterrows():
            index_hash = self.converter.get_index_value_hash(row, self.config.index_column)
            if index_hash and index_hash in current_index:
                # è¦†ç›–ç°æœ‰è¡Œçš„æŒ‡å®šåˆ—
                current_row_idx = current_index[index_hash]
                if current_row_idx not in update_data_map:
                    update_data_map[current_row_idx] = {}
                
                # åªè¦†ç›–æŒ‡å®šåˆ—
                for col in self.config.selective_sync.columns:
                    if col in df.columns:
                        update_data_map[current_row_idx][col] = row[col]
            else:
                # å…¨æ–°è¡Œ
                new_rows.append(row)
        
        self.logger.info(f"é€‰æ‹©æ€§åˆ—è¦†ç›–è®¡åˆ’: è¦†ç›– {len(update_data_map)} è¡Œçš„æŒ‡å®šåˆ—ï¼Œæ–°å¢ {len(new_rows)} è¡Œ")
        
        success = True
        
        # æ‰§è¡Œé€‰æ‹©æ€§åˆ—è¦†ç›–æ›´æ–°
        if update_data_map:
            success = self._update_selective_columns(current_df, update_data_map)
        
        # è¿½åŠ æ–°è¡Œï¼ˆå¦‚æœæœ‰ï¼‰
        if new_rows and success:
            # å¯¹äºæ–°è¡Œï¼Œä¹Ÿåº”è¯¥åªåŒ…å«é€‰æ‹©æ€§åˆ—
            if self.config.selective_sync.enabled and self.config.selective_sync.columns:
                success = self._append_selective_columns(pd.DataFrame(new_rows))
            else:
                # å¸¸è§„è¿½åŠ 
                new_df = pd.DataFrame(new_rows)
                new_values = self.converter.df_to_values(new_df, include_headers=False)
                
                if isinstance(self.api, SheetAPI) and self.config.spreadsheet_token and self.config.sheet_id:
                    success = self.api.append_sheet_data(
                        self.config.spreadsheet_token,
                        self.config.sheet_id,
                        new_values,
                        self.config.batch_size,
                        self.config.rate_limit_delay
                    )
        
        return success
    
    def sync_clone(self, df: pd.DataFrame) -> bool:
        """å…‹éš†åŒæ­¥ï¼šæ¸…ç©ºå…¨éƒ¨ï¼Œç„¶åæ–°å¢å…¨éƒ¨"""
        self.logger.info("å¼€å§‹å…‹éš†åŒæ­¥...")
        
        if self.config.target_type == TargetType.BITABLE:
            return self._sync_clone_bitable(df)
        else:  # SHEET
            return self._sync_clone_sheet(df)
    
    def _sync_clone_bitable(self, df: pd.DataFrame) -> bool:
        """å¤šç»´è¡¨æ ¼å…‹éš†åŒæ­¥"""
        # è·å–æ‰€æœ‰ç°æœ‰è®°å½•
        existing_records = self.get_all_bitable_records()
        existing_record_ids = [record["record_id"] for record in existing_records]
        
        self.logger.info(f"å…‹éš†åŒæ­¥è®¡åˆ’: åˆ é™¤ {len(existing_record_ids)} æ¡å·²æœ‰è®°å½•ï¼Œç„¶åæ–°å¢ {len(df)} æ¡è®°å½•")
        
        # åˆ é™¤æ‰€æœ‰è®°å½•
        delete_success = True
        if existing_record_ids and isinstance(self.api, BitableAPI) and self.config.app_token and self.config.table_id:
            delete_success = self.process_in_batches(
                existing_record_ids, self.config.batch_size,
                self.api.batch_delete_records,
                self.config.app_token, self.config.table_id
            )
        
        # æ–°å¢å…¨éƒ¨è®°å½•
        field_types = self.get_field_types()
        new_records = self.converter.df_to_records(df, field_types)
        create_success = False
        if isinstance(self.api, BitableAPI) and self.config.app_token and self.config.table_id:
            create_success = self.process_in_batches(
                new_records, self.config.batch_size,
                self.api.batch_create_records,
                self.config.app_token, self.config.table_id
            )
        
        return delete_success and create_success
    
    def _sync_clone_sheet(self, df: pd.DataFrame) -> bool:
        """ç”µå­è¡¨æ ¼å…‹éš†åŒæ­¥ - ä½¿ç”¨ä¼˜åŒ–APIç­–ç•¥"""
        # è½¬æ¢æ•°æ®æ ¼å¼
        values = self.converter.df_to_values(df)
        
        self.logger.info(f"å…‹éš†åŒæ­¥è®¡åˆ’: æ¸…ç©ºç°æœ‰æ•°æ®ï¼Œæ–°å¢ {len(df)} è¡Œ")
        self.logger.info(f"ä½¿ç”¨write_sheet_dataè¿›è¡Œå…‹éš†å†™å…¥")
        
        # é¦–å…ˆæ¸…ç©ºè¡¨æ ¼çš„ä¸€ä¸ªå¤§èŒƒå›´
        if isinstance(self.api, SheetAPI) and self.config.spreadsheet_token and self.config.sheet_id:
            self.logger.info("æ¸…ç©ºç°æœ‰æ•°æ®...")
            # æ³¨æ„ï¼šè¿™é‡Œçš„èŒƒå›´å¯èƒ½éœ€è¦æ ¹æ®å®é™…æœ€å¤§æ•°æ®é‡è°ƒæ•´ï¼Œæˆ–è€…å…ˆè·å–è¡¨æ ¼å…ƒæ•°æ®
            clear_success = self.api.clear_sheet_data(self.config.spreadsheet_token, self.config.sheet_id, "A1:ZZZ500000")
            if not clear_success:
                self.logger.error("æ¸…ç©ºç”µå­è¡¨æ ¼å¤±è´¥ï¼Œç»ˆæ­¢å…‹éš†åŒæ­¥")
                return False

            # ä½¿ç”¨å¢å¼ºçš„å†™å…¥æ–¹æ³•
            write_success = self.api.write_sheet_data(
                self.config.spreadsheet_token,
                self.config.sheet_id,
                values,
                self.config.batch_size,
                80,  # col_batch_size
                self.config.rate_limit_delay
            )
        else:
            write_success = False
        
        # æ•°æ®å†™å…¥æˆåŠŸåï¼Œå†åº”ç”¨æ™ºèƒ½å­—æ®µé…ç½®
        if write_success:
            if not self._setup_sheet_intelligence(df):
                self.logger.warning("æ™ºèƒ½å­—æ®µé…ç½®å¤±è´¥ï¼Œä½†æ•°æ®åŒæ­¥å·²å®Œæˆ")
        
        return write_success
    
    def _setup_sheet_intelligence(self, df: pd.DataFrame) -> bool:
        """
        ä¸ºç”µå­è¡¨æ ¼è®¾ç½®æ™ºèƒ½å­—æ®µé…ç½®
        
        Args:
            df: æ•°æ®DataFrame
            
        Returns:
            æ˜¯å¦è®¾ç½®æˆåŠŸ
        """
        if self.config.target_type != TargetType.SHEET:
            return True
        
        if not isinstance(self.api, SheetAPI):
            self.logger.error("å†…éƒ¨é€»è¾‘é”™è¯¯: _setup_sheet_intelligence åº”è¯¥åªè¢« SheetAPI è°ƒç”¨")
            return False
        
        # ä¸åŒç­–ç•¥çš„é…ç½®èŒƒå›´ä¸åŒ
        strategy_name = self.config.field_type_strategy.value
        self.logger.info(f"å¼€å§‹ç”µå­è¡¨æ ¼æ™ºèƒ½å­—æ®µé…ç½® ({strategy_name}ç­–ç•¥)...")
        
        # rawç­–ç•¥ï¼šä¸åº”ç”¨ä»»ä½•æ ¼å¼åŒ–ï¼Œç›´æ¥è¿”å›æˆåŠŸ
        if strategy_name == 'raw':
            self.logger.info("rawç­–ç•¥ï¼šè·³è¿‡æ‰€æœ‰æ ¼å¼åŒ–ï¼Œä¿æŒåŸå§‹æ•°æ®")
            return True
        
        # ç”Ÿæˆå­—æ®µé…ç½®
        field_config = self.converter.generate_sheet_field_config(
            df, self.config.field_type_strategy.value, self.config
        )
        
        success = True
        
        # 1. é…ç½®ä¸‹æ‹‰åˆ—è¡¨ (baseç­–ç•¥è·³è¿‡)
        if strategy_name != 'base':
            for dropdown_config in field_config['dropdown_configs']:
                column_name = dropdown_config['column']
                
                # è®¡ç®—åˆ—çš„ç»å¯¹ä½ç½®
                start_col_num = self.api.column_letter_to_number(self.config.start_column)
                col_index_in_df = list(df.columns).index(column_name)
                actual_col_num = start_col_num + col_index_in_df
                col_letter = self.api.column_number_to_letter(actual_col_num)
                
                # è®¡ç®—è¡Œçš„ç»å¯¹èŒƒå›´ (æ•°æ®è¡Œï¼Œä¸å«è¡¨å¤´)
                start_data_row = self.config.start_row + 1
                end_data_row = self.config.start_row + len(df)
                
                # ä»…åœ¨æœ‰æ•°æ®è¡Œæ—¶æ‰è®¾ç½®èŒƒå›´
                if end_data_row >= start_data_row:
                    range_str = f"{self.config.sheet_id}!{col_letter}{start_data_row}:{col_letter}{end_data_row}"
                else:
                    self.logger.warning(f"åˆ— '{column_name}' æ²¡æœ‰æ•°æ®è¡Œï¼Œè·³è¿‡ä¸‹æ‹‰åˆ—è¡¨è®¾ç½®")
                    continue
                
                # ç¡®ä¿ä½¿ç”¨SheetAPIå¹¶æ£€æŸ¥token
                if not isinstance(self.api, SheetAPI):
                    self.logger.error("APIç±»å‹ä¸åŒ¹é…ï¼Œéœ€è¦SheetAPI")
                    continue
                    
                if not self.config.spreadsheet_token:
                    self.logger.error("ç”µå­è¡¨æ ¼Tokenä¸ºç©º")
                    continue
                
                # è®¾ç½®ä¸‹æ‹‰åˆ—è¡¨
                dropdown_success = self.api.set_dropdown_validation(
                    self.config.spreadsheet_token,
                    range_str,
                    dropdown_config['options'],
                    dropdown_config['multiple'],
                    dropdown_config['colors']
                )
                
                if dropdown_success:
                    self.logger.info(f"æˆåŠŸä¸ºåˆ— '{column_name}' è®¾ç½®ä¸‹æ‹‰åˆ—è¡¨")
                else:
                    self.logger.error(f"ä¸ºåˆ— '{column_name}' è®¾ç½®ä¸‹æ‹‰åˆ—è¡¨å¤±è´¥")
                    # ä¸è®¾ç½®success = Falseï¼Œå…è®¸ç»§ç»­å…¶ä»–åˆ—çš„æ“ä½œ
        else:
            self.logger.info("baseç­–ç•¥è·³è¿‡ä¸‹æ‹‰åˆ—è¡¨é…ç½®")
        
        # 2. é…ç½®æ—¥æœŸæ ¼å¼
        if field_config['date_columns'] and isinstance(self.api, SheetAPI) and self.config.spreadsheet_token:
            date_ranges = []
            for column_name in field_config['date_columns']:
                start_col_num = self.api.column_letter_to_number(self.config.start_column)
                col_index_in_df = list(df.columns).index(column_name)
                actual_col_num = start_col_num + col_index_in_df
                col_letter = self.api.column_number_to_letter(actual_col_num)

                start_data_row = self.config.start_row + 1
                end_data_row = self.config.start_row + len(df)
                
                if end_data_row >= start_data_row:
                    range_str = f"{self.config.sheet_id}!{col_letter}{start_data_row}:{col_letter}{end_data_row}"
                    date_ranges.append(range_str)
            
            # è®¾ç½®æ—¥æœŸæ ¼å¼
            date_success = self.api.set_date_format(
                self.config.spreadsheet_token,
                date_ranges,
                "yyyy/MM/dd"
            )
            
            if date_success:
                self.logger.info(f"æˆåŠŸä¸º {len(date_ranges)} ä¸ªæ—¥æœŸåˆ—è®¾ç½®æ ¼å¼")
            else:
                self.logger.error("è®¾ç½®æ—¥æœŸæ ¼å¼å¤±è´¥")
                # ä¸è®¾ç½®success = Falseï¼Œå…è®¸ç»§ç»­å…¶ä»–æ“ä½œ
        
        # 3. é…ç½®æ•°å­—æ ¼å¼
        if field_config['number_columns'] and isinstance(self.api, SheetAPI) and self.config.spreadsheet_token:
            number_ranges = []
            for column_name in field_config['number_columns']:
                start_col_num = self.api.column_letter_to_number(self.config.start_column)
                col_index_in_df = list(df.columns).index(column_name)
                actual_col_num = start_col_num + col_index_in_df
                col_letter = self.api.column_number_to_letter(actual_col_num)
                
                start_data_row = self.config.start_row + 1
                end_data_row = self.config.start_row + len(df)

                if end_data_row >= start_data_row:
                    range_str = f"{self.config.sheet_id}!{col_letter}{start_data_row}:{col_letter}{end_data_row}"
                    number_ranges.append(range_str)
            
            # è®¾ç½®æ•°å­—æ ¼å¼
            number_success = self.api.set_number_format(
                self.config.spreadsheet_token,
                number_ranges,
                "#,##0.00"
            )
            
            if number_success:
                self.logger.info(f"æˆåŠŸä¸º {len(number_ranges)} ä¸ªæ•°å­—åˆ—è®¾ç½®æ ¼å¼")
            else:
                self.logger.error("è®¾ç½®æ•°å­—æ ¼å¼å¤±è´¥")
                # ä¸è®¾ç½®success = Falseï¼Œå…è®¸ç»§ç»­å…¶ä»–æ“ä½œ
        
        # è¾“å‡ºé…ç½®æ‘˜è¦
        dropdown_count = len(field_config['dropdown_configs']) if strategy_name != 'base' else 0
        date_count = len(field_config['date_columns'])
        number_count = len(field_config['number_columns'])
        total_configs = dropdown_count + date_count + number_count
        
        if total_configs > 0:
            config_summary = []
            if dropdown_count > 0:
                config_summary.append(f"{dropdown_count}ä¸ªä¸‹æ‹‰åˆ—è¡¨")
            if date_count > 0:
                config_summary.append(f"{date_count}ä¸ªæ—¥æœŸæ ¼å¼")
            if number_count > 0:
                config_summary.append(f"{number_count}ä¸ªæ•°å­—æ ¼å¼")
            
            self.logger.info(f"æ™ºèƒ½å­—æ®µé…ç½®å®Œæˆ: {', '.join(config_summary)}")
        else:
            self.logger.info("æœªæ£€æµ‹åˆ°éœ€è¦æ™ºèƒ½é…ç½®çš„å­—æ®µ")
        
        return success
    
    def sync(self, df: pd.DataFrame) -> bool:
        """æ‰§è¡ŒåŒæ­¥"""
        target_name = "å¤šç»´è¡¨æ ¼" if self.config.target_type == TargetType.BITABLE else "ç”µå­è¡¨æ ¼"
        self.logger.info(f"å¼€å§‹æ‰§è¡Œ {target_name} {self.config.sync_mode.value} åŒæ­¥æ¨¡å¼")
        self.logger.info(f"æ•°æ®æº: {len(df)} è¡Œ x {len(df.columns)} åˆ—")
        
        # é‡ç½®è½¬æ¢ç»Ÿè®¡
        self.converter.reset_stats()
        
        # å¤šç»´è¡¨æ ¼æ¨¡å¼éœ€è¦ç¡®ä¿å­—æ®µå­˜åœ¨
        if self.config.target_type == TargetType.BITABLE:
            success, field_types = self.ensure_fields_exist(df)
            if not success:
                self.logger.error("å­—æ®µåˆ›å»ºå¤±è´¥ï¼ŒåŒæ­¥ç»ˆæ­¢")
                return False
            
            self.logger.info(f"è·å–åˆ° {len(field_types)} ä¸ªå­—æ®µçš„ç±»å‹ä¿¡æ¯")
            
            # æ˜¾ç¤ºå­—æ®µç±»å‹æ˜ å°„æ‘˜è¦
            self._show_field_analysis_summary(df, field_types)
            
            # é¢„æ£€æŸ¥ï¼šåˆ†ææ•°æ®ä¸å­—æ®µç±»å‹çš„åŒ¹é…æƒ…å†µ
            self.logger.info("\nğŸ” æ­£åœ¨åˆ†ææ•°æ®ä¸å­—æ®µç±»å‹åŒ¹é…æƒ…å†µ...")
            mismatch_warnings = []
            sample_size = min(50, len(df))  # æ£€æŸ¥å‰50è¡Œä½œä¸ºæ ·æœ¬
            
            for _, row in df.head(sample_size).iterrows():
                for col_name, value in row.to_dict().items():
                    if pd.notnull(value) and col_name in field_types:
                        field_type = field_types[col_name]
                        # ç®€å•çš„ç±»å‹ä¸åŒ¹é…æ£€æµ‹
                        if field_type == 2 and isinstance(value, str):  # æ•°å­—å­—æ®µä½†æ˜¯å­—ç¬¦ä¸²å€¼
                            if not self.converter._is_number_string(str(value).strip()):
                                mismatch_warnings.append(f"å­—æ®µ '{col_name}' æ˜¯æ•°å­—ç±»å‹ï¼Œä½†åŒ…å«éæ•°å­—å€¼: '{value}'")
                        elif field_type == 5 and isinstance(value, str):  # æ—¥æœŸå­—æ®µä½†æ˜¯å­—ç¬¦ä¸²å€¼
                            if not (self.converter._is_timestamp_string(str(value)) or self.converter._is_date_string(str(value))):
                                mismatch_warnings.append(f"å­—æ®µ '{col_name}' æ˜¯æ—¥æœŸç±»å‹ï¼Œä½†åŒ…å«éæ—¥æœŸå€¼: '{value}'")
            
            if mismatch_warnings:
                unique_warnings = list(set(mismatch_warnings[:10]))  # æ˜¾ç¤ºå‰10ä¸ªå”¯ä¸€è­¦å‘Š
                self.logger.warning(f"å‘ç° {len(set(mismatch_warnings))} ç§æ•°æ®ç±»å‹ä¸åŒ¹é…æƒ…å†µï¼ˆæ ·æœ¬æ£€æŸ¥ï¼‰:")
                for warning in unique_warnings:
                    self.logger.warning(f"  â€¢ {warning}")
                self.logger.info("ç¨‹åºå°†è‡ªåŠ¨è¿›è¡Œå¼ºåˆ¶ç±»å‹è½¬æ¢...")
            else:
                self.logger.info("âœ… æ•°æ®ç±»å‹åŒ¹é…è‰¯å¥½")
        
        # æ ¹æ®åŒæ­¥æ¨¡å¼æ‰§è¡Œå¯¹åº”æ“ä½œ
        sync_result = False
        if self.config.sync_mode == SyncMode.FULL:
            sync_result = self.sync_full(df)
        elif self.config.sync_mode == SyncMode.INCREMENTAL:
            sync_result = self.sync_incremental(df)
        elif self.config.sync_mode == SyncMode.OVERWRITE:
            sync_result = self.sync_overwrite(df)
        elif self.config.sync_mode == SyncMode.CLONE:
            sync_result = self.sync_clone(df)
        else:
            self.logger.error(f"ä¸æ”¯æŒçš„åŒæ­¥æ¨¡å¼: {self.config.sync_mode}")
            return False
        
        # è¾“å‡ºè½¬æ¢ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»…å¤šç»´è¡¨æ ¼æ¨¡å¼ï¼‰
        if self.config.target_type == TargetType.BITABLE:
            self.converter.report_conversion_stats()
        
        return sync_result
    
    def _show_field_analysis_summary(self, df: pd.DataFrame, field_types: Dict[str, int]):
        """æ˜¾ç¤ºå­—æ®µåˆ†ææ‘˜è¦"""
        self.logger.info("\nğŸ“‹ å­—æ®µç±»å‹æ˜ å°„æ‘˜è¦:")
        self.logger.info("-" * 50)
        
        for col_name in df.columns:
            if col_name in field_types:
                field_type = field_types[col_name]
                type_name = self.converter.get_field_type_name(field_type)
                self.logger.info(f"  {col_name} â†’ {type_name} (ç±»å‹ç : {field_type})")
            else:
                self.logger.warning(f"  {col_name} â†’ æœªçŸ¥å­—æ®µç±»å‹")
                
        self.logger.info("-" * 50)