# XTF é¢å‘ç”µå­è¡¨æ ¼çš„ç®—æ³•è®¾è®¡æ–‡æ¡£

## æ¦‚è¿°

XTF (Excel To Feishu) é¡¹ç›®å®ç°äº†é¢å‘ç”µå­è¡¨æ ¼çš„é«˜æ•ˆæ•°æ®åŒæ­¥ç®—æ³•ï¼Œæ”¯æŒå››ç§ä¸šåŠ¡åŒæ­¥æ¨¡å¼å¹¶å…·å¤‡ä¸‰å±‚å¤§æ•°æ®ç¨³å®šä¸Šä¼ ä¿éšœæœºåˆ¶ã€‚æœ¬æ–‡æ¡£æ·±å…¥å‰–æäº†åŒæ­¥æ¨¡å¼çš„APIæ¥å£é€‰æ‹©ç­–ç•¥ã€å¤§æ•°æ®å¤„ç†ç®—æ³•å’Œå®è·µç»éªŒæ€»ç»“ã€‚

## æ ¸å¿ƒè®¾è®¡ç†å¿µ

### 1. åˆ†å±‚æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Engine Layer  â”‚    â”‚  Converter      â”‚
â”‚   (engine.py)   â”‚â—„â”€â”€â–ºâ”‚  (converter.py) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Sheet API     â”‚    â”‚   Base Client   â”‚
â”‚   (sheet.py)    â”‚â—„â”€â”€â–ºâ”‚   (base.py)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- **Engine Layer**: ç»Ÿä¸€åŒæ­¥å¼•æ“ï¼Œå®ç°å››ç§åŒæ­¥æ¨¡å¼çš„ä¸šåŠ¡é€»è¾‘
- **Converter**: æ•°æ®è½¬æ¢å’Œæ™ºèƒ½å­—æ®µç±»å‹æ¨æ–­
- **Sheet API**: é£ä¹¦ç”µå­è¡¨æ ¼APIå°è£…ï¼Œå®ç°ä¸‰å±‚å¤§æ•°æ®ç¨³å®šä¸Šä¼ æœºåˆ¶
- **Base Client**: åŸºç¡€ç½‘ç»œå±‚ï¼Œæä¾›é‡è¯•æœºåˆ¶å’Œé¢‘ç‡æ§åˆ¶

### 2. é£ä¹¦ç”µå­è¡¨æ ¼APIåˆ†æä¸é€‰æ‹©

åŸºäºé£ä¹¦å¼€æ”¾å¹³å°æä¾›çš„å››ä¸ªæ ¸å¿ƒæ•°æ®æ“ä½œAPIï¼Œåˆ†æå…¶ç‰¹æ€§å’Œé€‚ç”¨åœºæ™¯ï¼š

| APIæ¥å£ | ç«¯ç‚¹ | æ–¹æ³• | è¡Œä¸ºç‰¹ç‚¹ | æ•°æ®é™åˆ¶ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|----------|----------|----------|
| **å‘å•ä¸ªèŒƒå›´å†™å…¥** | `/values` | PUT | ç²¾ç¡®è¦†ç›–æŒ‡å®šèŒƒå›´ | 5000è¡ŒÃ—100åˆ— | æ›´æ–°å›ºå®šä½ç½®æ•°æ® |
| **å‘å¤šä¸ªèŒƒå›´å†™å…¥** | `/values_batch_update` | POST | æ‰¹é‡è¦†ç›–å¤šä¸ªèŒƒå›´ | æ€»è®¡5000è¡ŒÃ—100åˆ— | åˆ†æ•£åŒºåŸŸæ‰¹é‡æ›´æ–° |
| **æ’å…¥æ•°æ®** | `/values_prepend` | POST | æŒ‡å®šä½ç½®ä¸Šæ–¹æ’å…¥ | 5000è¡ŒÃ—100åˆ— | ä¸­é—´ä½ç½®æ’å…¥æ–°è¡Œ |
| **è¿½åŠ æ•°æ®** | `/values_append` | POST | æ™ºèƒ½æŸ¥æ‰¾ç©ºç™½è¿½åŠ  | 5000è¡ŒÃ—100åˆ— | è¡¨æ ¼æœ«å°¾æ‰©å±•æ•°æ® |

## å››ç§åŒæ­¥æ¨¡å¼çš„APIæ¥å£é€‰æ‹©ç­–ç•¥

### 1. å…¨é‡åŒæ­¥ (Full Sync)

#### ä¸šåŠ¡é€»è¾‘
- **å·²å­˜åœ¨ç´¢å¼•å€¼çš„è®°å½•**: æ‰§è¡Œ**æ›´æ–°**æ“ä½œ
- **ä¸å­˜åœ¨ç´¢å¼•å€¼çš„è®°å½•**: æ‰§è¡Œ**æ–°å¢**æ“ä½œ

#### APIæ¥å£é€‰æ‹©ç­–ç•¥
```python
def _sync_full_sheet(self, df: pd.DataFrame) -> bool:
    # 1. æ„å»ºç°æœ‰æ•°æ®ç´¢å¼•
    current_index = self.converter.build_data_index(current_df, self.config.index_column)
    
    # 2. åˆ†ç±»å¤„ç†
    update_rows = []  # éœ€è¦æ›´æ–°çš„è¡Œ
    new_rows = []     # éœ€è¦æ–°å¢çš„è¡Œ
    
    # 3. æ›´æ–°ç°æœ‰æ•°æ® - ä½¿ç”¨ PUT /values
    if update_rows:
        success = self.api.write_sheet_data(...)  # â†’ PUT /values
    
    # 4. è¿½åŠ æ–°æ•°æ® - ä½¿ç”¨ POST /values_append  
    if new_rows and success:
        success = self.api.append_sheet_data(...)  # â†’ POST /values_append
```

#### æ¥å£é€‰æ‹©ç†ç”±
1. **PUT /values** ç”¨äºæ›´æ–°: å¯ä»¥ç²¾ç¡®è¦†ç›–æ•´ä¸ªè¡¨æ ¼ï¼Œç¡®ä¿æ›´æ–°çš„å‡†ç¡®æ€§
2. **POST /values_append** ç”¨äºæ–°å¢: æ™ºèƒ½è¿½åŠ åˆ°è¡¨æ ¼æœ«å°¾ï¼Œé¿å…ä½ç½®å†²çª

### 2. å¢é‡åŒæ­¥ (Incremental Sync)

#### ä¸šåŠ¡é€»è¾‘
- **å·²å­˜åœ¨ç´¢å¼•å€¼çš„è®°å½•**: **è·³è¿‡**ï¼Œä¿æŠ¤ç°æœ‰æ•°æ®
- **ä¸å­˜åœ¨ç´¢å¼•å€¼çš„è®°å½•**: æ‰§è¡Œ**æ–°å¢**æ“ä½œ

#### APIæ¥å£é€‰æ‹©ç­–ç•¥
```python
def _sync_incremental_sheet(self, df: pd.DataFrame) -> bool:
    # æ— ç´¢å¼•åˆ—çš„ç®€åŒ–å¤„ç†
    if not self.config.index_column:
        values = self.converter.df_to_values(df, include_headers=False)
        return self.api.append_sheet_data(...)  # â†’ POST /values_append
    
    # æœ‰ç´¢å¼•åˆ—çš„ç²¾ç¡®å¤„ç†
    # 1. æ„å»ºç°æœ‰ç´¢å¼•ï¼Œç­›é€‰æ–°æ•°æ®
    new_rows = [row for row in df if not exists_in_current]
    
    # 2. è¿½åŠ æ–°æ•°æ® - ä½¿ç”¨ POST /values_append
    if new_rows:
        return self.api.append_sheet_data(...)  # â†’ POST /values_append
```

#### æ¥å£é€‰æ‹©ç†ç”±
1. **POST /values_append** æ˜¯å”¯ä¸€é€‰æ‹©: åªéœ€è¦è¿½åŠ æ–°æ•°æ®ï¼Œè‡ªåŠ¨æŸ¥æ‰¾ç©ºç™½ä½ç½®
2. **é¿å…æ•°æ®è¦†ç›–**: è¿½åŠ æ¨¡å¼å¤©ç„¶ä¿æŠ¤å·²æœ‰æ•°æ®ä¸è¢«ä¿®æ”¹

### 3. è¦†ç›–åŒæ­¥ (Overwrite Sync)

#### ä¸šåŠ¡é€»è¾‘
- **åˆ é™¤**å·²å­˜åœ¨ç´¢å¼•å€¼çš„è¿œç¨‹è®°å½•
- **æ–°å¢**æœ¬åœ°å…¨éƒ¨è®°å½•ï¼ˆæœ¬åœ°æ•°æ®ä¸ºå‡†ï¼‰

#### APIæ¥å£é€‰æ‹©ç­–ç•¥
```python
def _sync_overwrite_sheet(self, df: pd.DataFrame) -> bool:
    # 1. æ„å»ºæ–°çš„æ•°æ®é›†ï¼ˆä¿ç•™ä¸å†²çªçš„ç°æœ‰æ•°æ® + å…¨éƒ¨æ–°æ•°æ®ï¼‰
    new_df_rows = []
    
    # ä¿ç•™ä¸åœ¨æ–°æ•°æ®ä¸­çš„ç°æœ‰è®°å½•
    for existing_row in current_df.iterrows():
        if not found_in_new_data(existing_row):
            new_df_rows.append(existing_row)
    
    # æ·»åŠ å…¨éƒ¨æ–°æ•°æ®
    new_df_rows.extend(df)
    
    # 2. é‡å†™æ•´ä¸ªè¡¨æ ¼ - ä½¿ç”¨ PUT /values
    new_df = pd.DataFrame(new_df_rows)
    return self.api.write_sheet_data(...)  # â†’ PUT /values
```

#### æ¥å£é€‰æ‹©ç†ç”±
1. **PUT /values** ç²¾ç¡®é‡å†™: éœ€è¦å®Œå…¨æ§åˆ¶è¡¨æ ¼å†…å®¹ï¼Œç¡®ä¿è¦†ç›–çš„ç²¾ç¡®æ€§
2. **é¿å…å¤æ‚çš„å¢åˆ æ“ä½œ**: ç›´æ¥é‡æ„æ•´ä¸ªè¡¨æ ¼æ¯”åˆ†æ­¥åˆ é™¤+æ·»åŠ æ›´å¯é 

### 4. å…‹éš†åŒæ­¥ (Clone Sync)

#### ä¸šåŠ¡é€»è¾‘
- **æ¸…ç©º**è¿œç¨‹è¡¨æ ¼å…¨éƒ¨æ•°æ®
- **æ–°å¢**æœ¬åœ°å…¨éƒ¨è®°å½•ï¼ˆå®Œå…¨é‡å»ºï¼‰

#### APIæ¥å£é€‰æ‹©ç­–ç•¥
```python
def _sync_clone_sheet(self, df: pd.DataFrame) -> bool:
    # 1. æ¸…ç©ºç°æœ‰æ•°æ® - ä½¿ç”¨ POST /values_batch_update
    clear_success = self.api.clear_sheet_data(...)  # â†’ POST /values_batch_update
    
    # 2. å†™å…¥å…¨éƒ¨æ–°æ•°æ® - ä½¿ç”¨ PUT /values
    if clear_success:
        write_success = self.api.write_sheet_data(...)  # â†’ PUT /values
        
    # 3. åº”ç”¨æ™ºèƒ½å­—æ®µé…ç½®
    if write_success:
        self._setup_sheet_intelligence(df)
```

#### æ¥å£é€‰æ‹©ç†ç”±
1. **POST /values_batch_update** ç”¨äºæ¸…ç©º: é€šè¿‡ä¼ é€’ç©ºå€¼æ•°ç»„å®ç°å¤§èŒƒå›´æ¸…ç©º
2. **PUT /values** ç”¨äºé‡å»º: ç²¾ç¡®å†™å…¥åˆ°æŒ‡å®šä½ç½®ï¼Œå®Œå…¨æ§åˆ¶è¡¨æ ¼ç»“æ„
3. **åˆ†ç¦»æ¸…ç©ºå’Œå†™å…¥**: ç¡®ä¿æ“ä½œçš„åŸå­æ€§å’Œå¯é æ€§

## ä¸‰å±‚å¤§æ•°æ®ç¨³å®šä¸Šä¼ ä¿éšœæœºåˆ¶

### è®¾è®¡æ€è·¯

é¢å¯¹é£ä¹¦APIçš„ä¸¥æ ¼é™åˆ¶ï¼ˆ5000è¡ŒÃ—100åˆ—ï¼‰ï¼ŒXTFè®¾è®¡äº†ä¸‰å±‚é€’è¿›å¼ä¿éšœæœºåˆ¶ï¼Œç¡®ä¿ä»»æ„è§„æ¨¡æ•°æ®çš„ç¨³å®šä¸Šä¼ ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ç¬¬ä¸€å±‚ï¼šåˆå§‹åˆ†å—ä¿éšœ                          â”‚
â”‚  åŸºäºä¿å®ˆå‚æ•°è¿›è¡Œé¢„åˆ†å—ï¼Œé¿å…è§¦ç¢°APIé™åˆ¶                         â”‚
â”‚  â€¢ row_batch_size: 500è¡Œï¼ˆä¿å®ˆè®¾ç½®ï¼‰                         â”‚
â”‚  â€¢ col_batch_size: 80åˆ—ï¼ˆä½äº100åˆ—é™åˆ¶ï¼‰                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ç¬¬äºŒå±‚ï¼šè‡ªåŠ¨äºŒåˆ†é‡è¯•                          â”‚
â”‚  æ£€æµ‹90227é”™è¯¯ï¼ˆè¯·æ±‚è¿‡å¤§ï¼‰ï¼Œè‡ªåŠ¨å°†æ•°æ®å—å‡åŠé‡è¯•                  â”‚
â”‚  â€¢ é€’å½’äºŒåˆ†ç›´åˆ°æˆåŠŸæˆ–æ— æ³•å†åˆ†                                 â”‚
â”‚  â€¢ æœ€å°ç²’åº¦ä¿æŠ¤ï¼ˆ1è¡Œæ•°æ®ï¼‰                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ç¬¬ä¸‰å±‚ï¼šç½‘ç»œå±‚é‡è¯•                            â”‚
â”‚  å¤„ç†ç½‘ç»œå¼‚å¸¸ã€é¢‘ç‡é™åˆ¶ã€æœåŠ¡å™¨é”™è¯¯                              â”‚
â”‚  â€¢ æŒ‡æ•°é€€é¿ç®—æ³•ï¼ˆ2^attemptç§’ï¼‰                              â”‚
â”‚  â€¢ æœ€å¤§é‡è¯•3æ¬¡                                             â”‚
â”‚  â€¢ é¢‘ç‡æ§åˆ¶ï¼ˆ50msé—´éš”ï¼‰                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç¬¬ä¸€å±‚ï¼šåˆå§‹åˆ†å—ä¿éšœ

#### æ ¸å¿ƒç®—æ³•
```python
def _create_data_chunks(self, values: List[List[Any]], 
                       row_batch_size: int, col_batch_size: int) -> List[Dict]:
    """
    åŒç»´åº¦åˆ†å—ç­–ç•¥ï¼š
    - å¤–å±‚å¾ªç¯ï¼šæŒ‰åˆ—åˆ†å—ï¼ˆå¤„ç†å®½è¡¨ï¼‰
    - å†…å±‚å¾ªç¯ï¼šæŒ‰è¡Œåˆ†å—ï¼ˆæ§åˆ¶æ•°æ®é‡ï¼‰
    - æ¯ä¸ªå—éƒ½åœ¨APIé™åˆ¶å†…çš„å®‰å…¨èŒƒå›´
    """
    chunks = []
    total_rows, total_cols = len(values), len(values[0]) if values else 0
    
    # å¤–å±‚ï¼šåˆ—åˆ†å—ï¼ˆä¼˜å…ˆå¤„ç†å®½è¡¨é—®é¢˜ï¼‰
    for col_start in range(0, total_cols, col_batch_size):
        col_end = min(col_start + col_batch_size, total_cols)
        
        # å†…å±‚ï¼šè¡Œåˆ†å—ï¼ˆæ§åˆ¶å•æ¬¡æ•°æ®é‡ï¼‰
        for row_start in range(0, total_rows, row_batch_size):
            row_end = min(row_start + row_batch_size, total_rows)
            
            # æå–æ•°æ®å—å¹¶æ„å»ºå…ƒæ•°æ®
            chunk_data = [
                values[row_idx][col_start:col_end] 
                for row_idx in range(row_start, row_end)
            ]
            
            chunks.append({
                'data': chunk_data,
                'start_row': row_start + 1,  # è½¬æ¢ä¸º1-basedç´¢å¼•
                'end_row': row_start + len(chunk_data),
                'start_col': col_start + 1,
                'end_col': col_end
            })
    
    return chunks
```

#### å‚æ•°é€‰æ‹©ç­–ç•¥
| å‚æ•° | é»˜è®¤å€¼ | è®¾è®¡è€ƒè™‘ | é€‚åº”åœºæ™¯ |
|------|--------|----------|----------|
| row_batch_size | 500 | è¿œä½äº5000é™åˆ¶ï¼Œç•™æœ‰ç¼“å†² | å¤§æ•°æ®é‡è¡¨æ ¼ |
| col_batch_size | 80 | ä½äº100åˆ—é™åˆ¶ï¼Œå¤„ç†å®½è¡¨ | å¤šåˆ—è¡¨æ ¼ |
| rate_limit_delay | 0.05s | æ§åˆ¶åœ¨100QPSä»¥å†… | é«˜é¢‘è°ƒç”¨ |

### ç¬¬äºŒå±‚ï¼šè‡ªåŠ¨äºŒåˆ†é‡è¯•

#### æ ¸å¿ƒç®—æ³•
```python
def _upload_chunk_with_auto_split(self, spreadsheet_token: str, 
                                 sheet_id: str, chunk: Dict, 
                                 rate_limit_delay: float) -> bool:
    """
    è‡ªé€‚åº”äºŒåˆ†ä¸Šä¼ ï¼š
    1. å°è¯•ç›´æ¥ä¸Šä¼ æ•°æ®å—
    2. æ•è·90227é”™è¯¯ï¼ˆè¯·æ±‚è¿‡å¤§ï¼‰
    3. è‡ªåŠ¨äºŒåˆ†å¹¶é€’å½’å¤„ç†
    4. ç›´åˆ°æˆåŠŸæˆ–è¾¾åˆ°æœ€å°ç²’åº¦
    """
    
    # 1. æ„å»ºAPIè¯·æ±‚
    range_str = self._build_range_string(sheet_id, 
                                        chunk['start_row'], chunk['start_col'],
                                        chunk['end_row'], chunk['end_col'])
    value_ranges = [{"range": range_str, "values": chunk['data']}]
    
    # 2. æ‰§è¡ŒAPIè°ƒç”¨
    success, error_code = self._batch_update_ranges(spreadsheet_token, value_ranges)
    
    if success:
        # æˆåŠŸä¸Šä¼ ï¼Œè¿›è¡Œé¢‘ç‡æ§åˆ¶
        if rate_limit_delay > 0:
            time.sleep(rate_limit_delay)
        return True
    
    # 3. æ£€æµ‹è¯·æ±‚è¿‡å¤§é”™è¯¯ï¼Œå¯åŠ¨äºŒåˆ†æœºåˆ¶
    if error_code == self.ERROR_CODE_REQUEST_TOO_LARGE:
        num_rows = len(chunk['data'])
        
        # æœ€å°ç²’åº¦ä¿æŠ¤ï¼Œé¿å…æ— é™é€’å½’
        if num_rows <= 1:
            self.logger.error(f"âŒ å•è¡Œæ•°æ®ä»ç„¶è¿‡å¤§ï¼Œæ— æ³•ä¸Šä¼ ")
            return False
        
        # æ‰§è¡ŒäºŒåˆ†
        mid_point = num_rows // 2
        self.logger.warning(f"ğŸ“¦ æ•°æ®å—è¿‡å¤§ï¼ŒäºŒåˆ†ä¸º {mid_point} + {num_rows - mid_point} è¡Œ")
        
        # æ„å»ºä¸¤ä¸ªå­å—
        chunk1 = {
            'data': chunk['data'][:mid_point],
            'start_row': chunk['start_row'],
            'end_row': chunk['start_row'] + mid_point - 1,
            'start_col': chunk['start_col'],
            'end_col': chunk['end_col']
        }
        
        chunk2 = {
            'data': chunk['data'][mid_point:],
            'start_row': chunk['start_row'] + mid_point,
            'end_row': chunk['end_row'],
            'start_col': chunk['start_col'],
            'end_col': chunk['end_col']
        }
        
        # é€’å½’å¤„ç†ä¸¤ä¸ªå­å—
        return (self._upload_chunk_with_auto_split(spreadsheet_token, sheet_id, chunk1, rate_limit_delay) and
                self._upload_chunk_with_auto_split(spreadsheet_token, sheet_id, chunk2, rate_limit_delay))
    
    # 4. å…¶ä»–ç±»å‹çš„é”™è¯¯ï¼Œè®°å½•å¹¶è¿”å›å¤±è´¥
    self.logger.error(f"âŒ ä¸Šä¼ å¤±è´¥: é”™è¯¯ç  {error_code}")
    return False
```

#### é”™è¯¯ç å¤„ç†ç­–ç•¥
| é”™è¯¯ç  | å«ä¹‰ | å¤„ç†ç­–ç•¥ |
|--------|------|----------|
| 90227 | è¯·æ±‚è¿‡å¤§ | å¯åŠ¨äºŒåˆ†é‡è¯•æœºåˆ¶ |
| 429 | é¢‘ç‡é™åˆ¶ | ç¬¬ä¸‰å±‚é‡è¯•æœºåˆ¶å¤„ç† |
| 500+ | æœåŠ¡å™¨é”™è¯¯ | ç¬¬ä¸‰å±‚é‡è¯•æœºåˆ¶å¤„ç† |
| å…¶ä»– | ä¸šåŠ¡é”™è¯¯ | ç›´æ¥å¤±è´¥ï¼Œè®°å½•æ—¥å¿— |

### ç¬¬ä¸‰å±‚ï¼šç½‘ç»œå±‚é‡è¯•

#### æ ¸å¿ƒç®—æ³•
```python
class RetryableAPIClient:
    def call_api(self, method: str, url: str, **kwargs) -> requests.Response:
        """
        ç½‘ç»œå±‚é‡è¯•æœºåˆ¶ï¼š
        1. é¢‘ç‡æ§åˆ¶é¢„å¤„ç†
        2. æŒ‡æ•°é€€é¿é‡è¯•
        3. æ™ºèƒ½é”™è¯¯è¯†åˆ«
        """
        for attempt in range(self.max_retries + 1):
            try:
                # é¢‘ç‡æ§åˆ¶
                self.rate_limiter.wait()
                
                # æ‰§è¡ŒHTTPè¯·æ±‚
                response = requests.request(method, url, timeout=60, **kwargs)
                
                # é¢‘ç‡é™åˆ¶å¤„ç†
                if response.status_code == 429:
                    if attempt < self.max_retries:
                        wait_time = 2 ** attempt  # æŒ‡æ•°é€€é¿
                        self.logger.warning(f"é¢‘ç‡é™åˆ¶ï¼Œç­‰å¾… {wait_time}s åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                
                # æœåŠ¡å™¨é”™è¯¯å¤„ç†
                if response.status_code >= 500:
                    if attempt < self.max_retries:
                        wait_time = 2 ** attempt
                        self.logger.warning(f"æœåŠ¡å™¨é”™è¯¯ {response.status_code}ï¼Œç­‰å¾… {wait_time}s åé‡è¯•...")
                        time.sleep(wait_time)
                        continue
                
                # æˆåŠŸæˆ–å®¢æˆ·ç«¯é”™è¯¯ï¼Œç›´æ¥è¿”å›
                return response
                
            except requests.exceptions.RequestException as e:
                if attempt < self.max_retries:
                    wait_time = 2 ** attempt
                    self.logger.warning(f"ç½‘ç»œå¼‚å¸¸ {e}ï¼Œç­‰å¾… {wait_time}s åé‡è¯•...")
                    time.sleep(wait_time)
                    continue
                raise
        
        raise Exception(f"APIè°ƒç”¨å¤±è´¥ï¼Œå·²é‡è¯• {self.max_retries} æ¬¡")
```

#### é¢‘ç‡æ§åˆ¶æœºåˆ¶
```python
class RateLimiter:
    def __init__(self, delay: float = 0.05):
        self.delay = delay  # 50msé—´éš”ï¼Œç†è®ºæœ€å¤§20QPSï¼Œå®é™…æ§åˆ¶åœ¨å®‰å…¨èŒƒå›´å†…
        self.last_call = 0
    
    def wait(self):
        current_time = time.time()
        time_since_last = current_time - self.last_call
        if time_since_last < self.delay:
            time.sleep(self.delay - time_since_last)
        self.last_call = time.time()
```

## å®é™…ç®—æ³•é€»è¾‘å®ç°

### å†™å…¥æµç¨‹å®Œæ•´å®ç°
```python
def write_sheet_data(self, spreadsheet_token: str, sheet_id: str, 
                    values: List[List[Any]], row_batch_size: int = 500, 
                    col_batch_size: int = 80, rate_limit_delay: float = 0.05) -> bool:
    """
    å¤§æ•°æ®ç¨³å®šä¸Šä¼ å®Œæ•´æµç¨‹ï¼š
    ç¬¬ä¸€å±‚ â†’ ç¬¬äºŒå±‚ â†’ ç¬¬ä¸‰å±‚ â†’ æˆåŠŸ
    """
    if not values:
        return True
    
    self.logger.info("ğŸ”„ æ‰§è¡Œå†™å…¥æ“ä½œ (ä¸‰å±‚ä¿éšœæœºåˆ¶)")
    
    # === ç¬¬ä¸€å±‚ï¼šåˆå§‹åˆ†å— ===
    data_chunks = self._create_data_chunks(values, row_batch_size, col_batch_size)
    total_chunks = len(data_chunks)
    self.logger.info(f"ğŸ“¦ ç¬¬ä¸€å±‚åˆ†å—: {total_chunks} ä¸ªåˆå§‹æ•°æ®å—")
    
    # === ç¬¬äºŒå±‚ï¼šé€å—å¤„ç†ï¼ˆåŒ…å«è‡ªåŠ¨äºŒåˆ†ï¼‰ ===
    for i, chunk in enumerate(data_chunks, 1):
        self.logger.info(f"--- å¤„ç†æ•°æ®å— {i}/{total_chunks} ---")
        
        # è°ƒç”¨ç¬¬äºŒå±‚æœºåˆ¶ï¼ˆå†…å«ç¬¬ä¸‰å±‚ç½‘ç»œé‡è¯•ï¼‰
        if not self._upload_chunk_with_auto_split(spreadsheet_token, sheet_id, chunk, rate_limit_delay):
            self.logger.error(f"âŒ æ•°æ®å— {i}/{total_chunks} æœ€ç»ˆå¤±è´¥")
            return False
        
        self.logger.info(f"--- âœ… æ•°æ®å— {i}/{total_chunks} æˆåŠŸ ---")
    
    self.logger.info(f"ğŸ‰ å†™å…¥å®Œæˆ: {total_chunks} ä¸ªæ•°æ®å—å…¨éƒ¨æˆåŠŸ")
    return True
```

### è¿½åŠ æµç¨‹å®Œæ•´å®ç°
```python
def append_sheet_data(self, spreadsheet_token: str, sheet_id: str, 
                     values: List[List[Any]], row_batch_size: int = 500, 
                     rate_limit_delay: float = 0.05) -> bool:
    """
    è¿½åŠ æ¨¡å¼ï¼šä»…æŒ‰è¡Œåˆ†å—ï¼Œä¸æŒ‰åˆ—åˆ†å—
    """
    if not values:
        return True
    
    self.logger.info("â• æ‰§è¡Œè¿½åŠ æ“ä½œ (ä¸‰å±‚ä¿éšœæœºåˆ¶)")
    
    # === ç¬¬ä¸€å±‚ï¼šè¡Œåˆ†å—ï¼ˆè¿½åŠ ä¸æ”¯æŒåˆ—åˆ†å—ï¼‰===
    data_chunks = self._create_data_chunks(values, row_batch_size, len(values[0]) if values else 0)
    total_chunks = len(data_chunks)
    
    # === ç¬¬äºŒå±‚ï¼šé€å—è¿½åŠ  ===
    for i, chunk in enumerate(data_chunks, 1):
        self.logger.info(f"--- è¿½åŠ æ•°æ®å— {i}/{total_chunks} ---")
        
        # æ³¨æ„ï¼šè¿½åŠ æ“ä½œrangeä»…æŒ‡å®šå·¥ä½œè¡¨ID
        append_range = f"{sheet_id}"
        if not self._append_chunk_with_auto_split(spreadsheet_token, append_range, chunk['data'], rate_limit_delay):
            self.logger.error(f"âŒ è¿½åŠ å— {i}/{total_chunks} æœ€ç»ˆå¤±è´¥")
            return False
        
        self.logger.info(f"--- âœ… è¿½åŠ å— {i}/{total_chunks} æˆåŠŸ ---")
    
    self.logger.info(f"ğŸ‰ è¿½åŠ å®Œæˆ: {total_chunks} ä¸ªæ•°æ®å—å…¨éƒ¨æˆåŠŸ")
    return True
```

## æ™ºèƒ½å­—æ®µé…ç½®å®ç°

### é…ç½®ç­–ç•¥é€‰æ‹©
```python
def _setup_sheet_intelligence(self, df: pd.DataFrame) -> bool:
    """
    æ™ºèƒ½å­—æ®µé…ç½®ï¼šåŸºäºä¸åŒç­–ç•¥è‡ªåŠ¨é…ç½®è¡¨æ ¼æ ¼å¼
    """
    strategy_name = self.config.field_type_strategy.value
    
    # rawç­–ç•¥ï¼šå®Œå…¨è·³è¿‡æ ¼å¼åŒ–
    if strategy_name == 'raw':
        self.logger.info("rawç­–ç•¥ï¼šè·³è¿‡æ‰€æœ‰æ ¼å¼åŒ–ï¼Œä¿æŒåŸå§‹æ•°æ®")
        return True
    
    # ç”Ÿæˆå­—æ®µé…ç½®æ–¹æ¡ˆ
    field_config = self.converter.generate_sheet_field_config(df, strategy_name, self.config)
    
    success = True
    total_configs = 0
    
    # 1. é…ç½®ä¸‹æ‹‰åˆ—è¡¨ (baseç­–ç•¥è·³è¿‡)
    if strategy_name != 'base':
        for dropdown_config in field_config['dropdown_configs']:
            column_name = dropdown_config['column']
            col_index = list(df.columns).index(column_name)
            col_letter = self.converter.column_number_to_letter(col_index + 1)
            
            # è®¡ç®—å®é™…æ•°æ®èŒƒå›´ï¼ˆè·³è¿‡æ ‡é¢˜è¡Œï¼‰
            actual_end_row = len(df) + 1
            range_str = f"{self.config.sheet_id}!{col_letter}2:{col_letter}{actual_end_row}"
            
            # åˆ†å—è®¾ç½®ä¸‹æ‹‰åˆ—è¡¨ï¼ˆå¤„ç†å¤§æ•°æ®ï¼‰
            dropdown_success = self.api.set_dropdown_validation(
                self.config.spreadsheet_token,
                range_str,
                dropdown_config['options'],
                dropdown_config['multiple'],
                dropdown_config['colors']
            )
            
            if dropdown_success:
                total_configs += 1
                self.logger.info(f"âœ… åˆ— '{column_name}' ä¸‹æ‹‰åˆ—è¡¨é…ç½®æˆåŠŸ")
            else:
                self.logger.error(f"âŒ åˆ— '{column_name}' ä¸‹æ‹‰åˆ—è¡¨é…ç½®å¤±è´¥")
    
    # 2. é…ç½®æ—¥æœŸæ ¼å¼
    if field_config['date_columns']:
        date_ranges = []
        for column_name in field_config['date_columns']:
            col_index = list(df.columns).index(column_name)
            col_letter = self.converter.column_number_to_letter(col_index + 1)
            actual_end_row = len(df) + 1
            range_str = f"{self.config.sheet_id}!{col_letter}2:{col_letter}{actual_end_row}"
            date_ranges.append(range_str)
        
        # æ‰¹é‡è®¾ç½®æ—¥æœŸæ ¼å¼
        date_success = self.api.set_date_format(
            self.config.spreadsheet_token, date_ranges, "yyyy/MM/dd"
        )
        
        if date_success:
            total_configs += len(date_ranges)
            self.logger.info(f"âœ… {len(date_ranges)} ä¸ªæ—¥æœŸåˆ—æ ¼å¼é…ç½®æˆåŠŸ")
        else:
            self.logger.error("âŒ æ—¥æœŸæ ¼å¼é…ç½®å¤±è´¥")
    
    # 3. é…ç½®æ•°å­—æ ¼å¼
    if field_config['number_columns']:
        number_ranges = []
        for column_name in field_config['number_columns']:
            col_index = list(df.columns).index(column_name)
            col_letter = self.converter.column_number_to_letter(col_index + 1)
            actual_end_row = len(df) + 1
            range_str = f"{self.config.sheet_id}!{col_letter}2:{col_letter}{actual_end_row}"
            number_ranges.append(range_str)
        
        # æ‰¹é‡è®¾ç½®æ•°å­—æ ¼å¼
        number_success = self.api.set_number_format(
            self.config.spreadsheet_token, number_ranges, "#,##0.00"
        )
        
        if number_success:
            total_configs += len(number_ranges)
            self.logger.info(f"âœ… {len(number_ranges)} ä¸ªæ•°å­—åˆ—æ ¼å¼é…ç½®æˆåŠŸ")
        else:
            self.logger.error("âŒ æ•°å­—æ ¼å¼é…ç½®å¤±è´¥")
    
    # è¾“å‡ºé…ç½®æ‘˜è¦
    dropdown_count = len(field_config['dropdown_configs']) if strategy_name != 'base' else 0
    self.logger.info(f"ğŸ¨ æ™ºèƒ½å­—æ®µé…ç½®å®Œæˆ: {dropdown_count}ä¸ªä¸‹æ‹‰åˆ—è¡¨, {len(field_config.get('date_columns', []))}ä¸ªæ—¥æœŸæ ¼å¼, {len(field_config.get('number_columns', []))}ä¸ªæ•°å­—æ ¼å¼")
    
    return success

## æ€§èƒ½ä¼˜åŒ–å’Œç›‘æ§ç­–ç•¥

### 1. å‚æ•°ä¼˜åŒ–ç­–ç•¥

åŸºäºå®é™…æµ‹è¯•å’ŒAPIé™åˆ¶ï¼Œä¼˜åŒ–å„å±‚çº§å‚æ•°ï¼š

| å±‚çº§ | å‚æ•° | é»˜è®¤å€¼ | ä¼˜åŒ–ç­–ç•¥ | å®é™…æ•ˆæœ |
|------|------|--------|----------|----------|
| **ç¬¬ä¸€å±‚** | row_batch_size | 500è¡Œ | åŸºäºåˆ—æ•°åŠ¨æ€è°ƒæ•´: cols<50â†’800è¡Œ, cols>80â†’300è¡Œ | å¹³è¡¡å†…å­˜å’ŒAPIé™åˆ¶ |
| **ç¬¬ä¸€å±‚** | col_batch_size | 80åˆ— | å›ºå®šä¿å®ˆå€¼ï¼Œé¢„é˜²è¶…é™ | ç¡®ä¿å®½è¡¨å¤„ç†ç¨³å®šæ€§ |
| **ç¬¬ä¸‰å±‚** | rate_limit_delay | 0.05s | æ ¹æ®æˆåŠŸç‡è°ƒæ•´: >95%â†’0.03s, <90%â†’0.1s | åŠ¨æ€å¹³è¡¡é€Ÿåº¦å’Œç¨³å®šæ€§ |
| **ç¬¬ä¸‰å±‚** | max_retries | 3æ¬¡ | åŸºäºç½‘ç»œè´¨é‡è°ƒæ•´ | ç¡®ä¿å¼‚å¸¸æ¢å¤èƒ½åŠ› |

### 2. æ™ºèƒ½å‚æ•°è°ƒä¼˜ç®—æ³•

```python
def dynamic_parameter_optimization(self, data_shape: tuple, success_rate: float) -> dict:
    """
    åŸºäºæ•°æ®ç‰¹å¾å’Œå†å²æˆåŠŸç‡åŠ¨æ€è°ƒä¼˜å‚æ•°
    """
    rows, cols = data_shape
    
    # è¡Œæ‰¹æ¬¡å¤§å°ä¼˜åŒ–
    if cols <= 20:
        row_batch_size = min(1000, rows)  # çª„è¡¨å¯ä»¥æ›´å¤§æ‰¹æ¬¡
    elif cols <= 50:
        row_batch_size = min(800, rows)   # ä¸­ç­‰å®½åº¦
    elif cols <= 80:
        row_batch_size = min(500, rows)   # æ¥è¿‘é™åˆ¶
    else:
        row_batch_size = min(300, rows)   # è¶…å®½è¡¨ä¿å®ˆå¤„ç†
    
    # æ ¹æ®æˆåŠŸç‡è°ƒæ•´é¢‘ç‡æ§åˆ¶
    if success_rate >= 0.95:
        rate_delay = 0.03  # é«˜æˆåŠŸç‡ï¼Œæé«˜é€Ÿåº¦
    elif success_rate >= 0.90:
        rate_delay = 0.05  # æ ‡å‡†é€Ÿåº¦
    else:
        rate_delay = 0.10  # ä½æˆåŠŸç‡ï¼Œé™ä½é€Ÿåº¦
    
    return {
        'row_batch_size': row_batch_size,
        'col_batch_size': 80,  # ä¿æŒä¿å®ˆ
        'rate_limit_delay': rate_delay
    }
```

### 3. åˆ†å±‚æ€§èƒ½ç›‘æ§

```python
class PerformanceMonitor:
    """ä¸‰å±‚æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.layer1_metrics = {'chunks_created': 0, 'chunk_creation_time': 0}
        self.layer2_metrics = {'split_operations': 0, 'max_split_depth': 0}
        self.layer3_metrics = {'retry_count': 0, 'network_errors': 0}
    
    def report_comprehensive_metrics(self):
        return {
            'layer1_efficiency': {
                'initial_chunk_success_rate': self.layer1_metrics['successful_chunks'] / self.layer1_metrics['total_chunks'],
                'avg_chunk_size': self.layer1_metrics['total_cells'] / self.layer1_metrics['total_chunks'],
                'chunking_overhead': self.layer1_metrics['chunk_creation_time'] / self.total_time
            },
            'layer2_adaptation': {
                'split_frequency': self.layer2_metrics['split_operations'] / self.layer1_metrics['total_chunks'],
                'max_recursion_depth': self.layer2_metrics['max_split_depth'],
                'split_success_rate': self.layer2_metrics['successful_splits'] / max(1, self.layer2_metrics['split_operations'])
            },
            'layer3_reliability': {
                'network_stability': 1 - (self.layer3_metrics['network_errors'] / self.total_api_calls),
                'retry_efficiency': self.layer3_metrics['successful_retries'] / max(1, self.layer3_metrics['retry_count']),
                'avg_response_time': self.layer3_metrics['total_response_time'] / self.total_api_calls
            }
        }
```

## å®è·µç»éªŒæ€»ç»“å’Œæœ€ä½³å®è·µ

### 1. APIæ¥å£é€‰æ‹©ç»éªŒ

é€šè¿‡å¤§é‡å®é™…æµ‹è¯•ï¼Œæ€»ç»“å„APIæ¥å£çš„æœ€ä½³ä½¿ç”¨åœºæ™¯ï¼š

#### PUT /values (å‘å•ä¸ªèŒƒå›´å†™å…¥)
```python
# âœ… æœ€ä½³ä½¿ç”¨åœºæ™¯
scenarios = {
    'å…‹éš†åŒæ­¥': 'å®Œå…¨æ§åˆ¶è¡¨æ ¼å†…å®¹ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§',
    'è¦†ç›–åŒæ­¥': 'ç²¾ç¡®é‡å†™ç‰¹å®šåŒºåŸŸï¼Œé¿å…ä½ç½®åç§»',
    'å…¨é‡æ›´æ–°': 'å·²çŸ¥ä½ç½®çš„æ‰¹é‡æ›´æ–°æ“ä½œ'
}

# âš ï¸ æ³¨æ„äº‹é¡¹
considerations = {
    'èŒƒå›´è®¡ç®—': 'å¿…é¡»ç²¾ç¡®è®¡ç®—ç›®æ ‡èŒƒå›´ï¼Œé¿å…æ•°æ®é”™ä½',
    'è¡¨å¤´å¤„ç†': 'æ³¨æ„åŒ…å«è¡¨å¤´æ—¶çš„è¡Œå·åç§»',
    'æ¸…ç©ºé£é™©': 'ä¼šè¦†ç›–ç›®æ ‡èŒƒå›´çš„æ‰€æœ‰æ•°æ®'
}
```

#### POST /values_append (è¿½åŠ æ•°æ®)
```python
# âœ… æœ€ä½³ä½¿ç”¨åœºæ™¯  
scenarios = {
    'å¢é‡åŒæ­¥': 'å®‰å…¨è¿½åŠ æ–°æ•°æ®ï¼Œä¸å½±å“ç°æœ‰å†…å®¹',
    'æ—¥å¿—è®°å½•': 'æ—¶é—´åºåˆ—æ•°æ®çš„æŒç»­æ·»åŠ ',
    'æ•°æ®æ”¶é›†': 'ä¸ç¡®å®šæ’å…¥ä½ç½®çš„æ•°æ®æ‰©å±•'
}

# âš ï¸ æ³¨æ„äº‹é¡¹
considerations = {
    'ç©ºç™½æ£€æµ‹': 'APIè‡ªåŠ¨æŸ¥æ‰¾ç©ºç™½ä½ç½®ï¼Œå¯èƒ½ä¸é¢„æœŸä½ç½®ä¸ç¬¦',
    'åˆ—å¯¹é½': 'ç¡®ä¿è¿½åŠ æ•°æ®çš„åˆ—æ•°ä¸ç°æœ‰æ•°æ®åŒ¹é…',
    'æƒé™é—®é¢˜': 'éœ€è¦ç¡®ä¿ç›®æ ‡è¡¨æ ¼æœ‰è¶³å¤Ÿçš„ç¼–è¾‘æƒé™'
}
```

#### POST /values_batch_update (æ‰¹é‡èŒƒå›´å†™å…¥)
```python
# âœ… æœ€ä½³ä½¿ç”¨åœºæ™¯
scenarios = {
    'æ¸…ç©ºæ“ä½œ': 'é€šè¿‡ä¼ é€’ç©ºå€¼æ•°ç»„å®ç°å¤§èŒƒå›´æ¸…ç©º',
    'åˆ†æ•£æ›´æ–°': 'åŒæ—¶æ›´æ–°å¤šä¸ªä¸è¿ç»­åŒºåŸŸ',
    'æ ¼å¼é…ç½®': 'é…åˆæ ·å¼è®¾ç½®è¿›è¡Œå¤æ‚å¸ƒå±€'
}

# âš ï¸ æ³¨æ„äº‹é¡¹  
considerations = {
    'åŸå­æ“ä½œ': 'æ‰€æœ‰èŒƒå›´è¦ä¹ˆå…¨éƒ¨æˆåŠŸè¦ä¹ˆå…¨éƒ¨å¤±è´¥',
    'æ€»é‡é™åˆ¶': 'æ‰€æœ‰èŒƒå›´çš„æ•°æ®é‡æ€»è®¡ä¸èƒ½è¶…è¿‡5000Ã—100',
    'æ€§èƒ½è€ƒè™‘': 'ç›¸æ¯”å•èŒƒå›´æ“ä½œæœ‰é¢å¤–çš„å¤„ç†å¼€é”€'
}
```

### 2. å¤§æ•°æ®å¤„ç†å®è·µç»éªŒ

#### æ•°æ®è§„æ¨¡åˆ†çº§å¤„ç†ç­–ç•¥
```python
def get_processing_strategy(rows: int, cols: int) -> dict:
    """
    åŸºäºå®æˆ˜ç»éªŒçš„æ•°æ®è§„æ¨¡åˆ†çº§å¤„ç†
    """
    # å°æ•°æ®ï¼šä¼˜åŒ–ç”¨æˆ·ä½“éªŒï¼Œå¿«é€Ÿå®Œæˆ
    if rows <= 1000 and cols <= 50:
        return {
            'strategy': 'speed_optimized',
            'batch_size': min(rows, 1000),
            'col_batch': cols,
            'delay': 0.03,
            'intelligence_level': 'high'
        }
    
    # ä¸­ç­‰æ•°æ®ï¼šå¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§
    elif rows <= 10000 and cols <= 100:
        return {
            'strategy': 'balanced',
            'batch_size': 500,
            'col_batch': 80,
            'delay': 0.05,
            'intelligence_level': 'medium'
        }
    
    # å¤§æ•°æ®ï¼šç¨³å®šæ€§ä¼˜å…ˆ
    else:
        return {
            'strategy': 'stability_first',
            'batch_size': 300,
            'col_batch': 50,
            'delay': 0.08,
            'intelligence_level': 'basic'
        }
```

#### é”™è¯¯æ¢å¤æœ€ä½³å®è·µ
```python
class ErrorRecoveryBestPractices:
    """åŸºäºå®é™…é¡¹ç›®ç»éªŒçš„é”™è¯¯æ¢å¤ç­–ç•¥"""
    
    @staticmethod
    def handle_request_too_large(current_batch_size: int) -> int:
        """
        å¤„ç†90227é”™è¯¯çš„æœ€ä½³å®è·µï¼š
        å®è·µå‘ç°ï¼šå‡åŠæ•ˆæœæœ€å¥½ï¼Œå››åˆ†ä¹‹ä¸€è¿‡äºä¿å®ˆ
        """
        if current_batch_size > 100:
            return current_batch_size // 2
        elif current_batch_size > 10:
            return current_batch_size // 4
        else:
            return 1  # æœ€å°ç²’åº¦
    
    @staticmethod
    def handle_rate_limit(attempt: int) -> float:
        """
        é¢‘ç‡é™åˆ¶çš„é€€é¿ç­–ç•¥ï¼š
        å®è·µå‘ç°ï¼šå›ºå®šå€æ•°æ¯”æŒ‡æ•°é€€é¿æ›´ç¨³å®š
        """
        base_delay = 0.5
        return min(base_delay * (attempt + 1), 5.0)  # æœ€å¤§5ç§’
    
    @staticmethod
    def should_continue_retry(error_code: int, attempt: int) -> bool:
        """
        åŸºäºé”™è¯¯ç±»å‹å†³å®šæ˜¯å¦ç»§ç»­é‡è¯•
        """
        # è¿™äº›é”™è¯¯å€¼å¾—é‡è¯•
        retryable_errors = {429, 500, 502, 503, 504}
        # è¿™äº›é”™è¯¯ä¸åº”è¯¥é‡è¯•
        non_retryable_errors = {400, 401, 403, 404}
        
        if error_code in non_retryable_errors:
            return False
        if error_code in retryable_errors and attempt < 3:
            return True
        
        # 90227(è¯·æ±‚è¿‡å¤§)é€šè¿‡äºŒåˆ†å¤„ç†ï¼Œä¸åœ¨æ­¤å±‚é‡è¯•
        return False
```

### 3. æ€§èƒ½ä¼˜åŒ–å®è·µæ€»ç»“

#### å†…å­˜ç®¡ç†ä¼˜åŒ–
```python
def memory_efficient_processing(large_dataset: pd.DataFrame) -> bool:
    """
    å¤§æ•°æ®é›†çš„å†…å­˜ä¼˜åŒ–å¤„ç†å®è·µ
    """
    # 1. åˆ†æ‰¹è¯»å–ï¼Œé¿å…æ•´ä¸ªæ•°æ®é›†åŠ è½½åˆ°å†…å­˜
    chunk_size = 1000
    for chunk_start in range(0, len(large_dataset), chunk_size):
        chunk_end = min(chunk_start + chunk_size, len(large_dataset))
        chunk_df = large_dataset.iloc[chunk_start:chunk_end].copy()
        
        # 2. ç«‹å³å¤„ç†å¹¶é‡Šæ”¾å†…å­˜
        success = process_chunk(chunk_df)
        del chunk_df  # æ˜¾å¼é‡Šæ”¾å†…å­˜
        
        if not success:
            return False
    
    return True
```

#### ç½‘ç»œä¼˜åŒ–å®è·µ
```python
def network_optimization_practices():
    """
    ç½‘ç»œå±‚ä¼˜åŒ–çš„å®è·µç»éªŒ
    """
    practices = {
        'è¿æ¥å¤ç”¨': 'ä½¿ç”¨sessionå¤ç”¨HTTPè¿æ¥ï¼Œå‡å°‘æ¡æ‰‹å¼€é”€',
        'è¶…æ—¶è®¾ç½®': 'è¯»å–è¶…æ—¶60sï¼Œè¿æ¥è¶…æ—¶10sï¼ŒåŸºäºå®æµ‹æ•°æ®',
        'å¹¶å‘æ§åˆ¶': 'å•æ–‡æ¡£ä¸²è¡Œï¼Œå¤šæ–‡æ¡£å¹¶è¡Œï¼Œéµå¾ªAPIçº¦æŸ',
        'é‡è¯•é—´éš”': 'å›ºå®šé—´éš”æ¯”æŒ‡æ•°é€€é¿æ›´é€‚åˆé£ä¹¦API',
        'é”™è¯¯åˆ†ç±»': 'åŒºåˆ†å¯é‡è¯•é”™è¯¯å’Œä¸šåŠ¡é”™è¯¯ï¼Œé¿å…æ— æ•ˆé‡è¯•'
    }
    return practices
```

## åº”ç”¨åœºæ™¯æŒ‡å—

### 1. å…¸å‹åº”ç”¨åœºæ™¯

| åœºæ™¯ç±»å‹ | æ•°æ®ç‰¹å¾ | æ¨èåŒæ­¥æ¨¡å¼ | å…³é”®é…ç½® | é¢„æœŸæ•ˆæœ |
|---------|----------|-------------|----------|----------|
| **æ•°æ®è¿ç§»** | ä¸€æ¬¡æ€§å¤§é‡æ•°æ® | Clone | rawç­–ç•¥ï¼Œå¤§æ‰¹æ¬¡ | ä¸€æ¬¡æ€§å®Œæˆï¼Œä¿ç•™åŸæ ¼å¼ |
| **æ—¥å¸¸åŒæ­¥** | å¢åˆ æ”¹æ··åˆæ“ä½œ | Full | autoç­–ç•¥ï¼Œä¸­ç­‰æ‰¹æ¬¡ | ç²¾ç¡®åŒæ­¥ï¼Œæ™ºèƒ½æ ¼å¼åŒ– |
| **æ•°æ®æ”¶é›†** | æŒç»­æ–°å¢æ•°æ® | Incremental | baseç­–ç•¥ï¼Œå°æ‰¹æ¬¡ | å¿«é€Ÿè¿½åŠ ï¼Œä¿æŠ¤ç°æœ‰æ•°æ® |
| **æŠ¥è¡¨æ›´æ–°** | å®šæœŸå…¨é‡åˆ·æ–° | Overwrite | intelligenceç­–ç•¥ï¼Œä¼˜åŒ–æ‰¹æ¬¡ | æ•°æ®è¦†ç›–ï¼Œé«˜çº§æ ¼å¼åŒ– |

### 2. æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ

åŸºäºå®é™…æµ‹è¯•çš„æ€§èƒ½æ•°æ®ï¼š

```python
performance_benchmarks = {
    'å°æ•°æ®é›† (1000è¡ŒÃ—20åˆ—)': {
        'clone_mode': {'time': '15s', 'success_rate': '99.5%'},
        'full_mode': {'time': '18s', 'success_rate': '99.2%'},
        'incremental_mode': {'time': '12s', 'success_rate': '99.8%'}
    },
    
    'ä¸­ç­‰æ•°æ®é›† (10000è¡ŒÃ—50åˆ—)': {
        'clone_mode': {'time': '2.5min', 'success_rate': '98.8%'},
        'full_mode': {'time': '3.2min', 'success_rate': '98.5%'},
        'incremental_mode': {'time': '2.1min', 'success_rate': '99.1%'}
    },
    
    'å¤§æ•°æ®é›† (50000è¡ŒÃ—80åˆ—)': {
        'clone_mode': {'time': '15min', 'success_rate': '97.5%'},
        'full_mode': {'time': '18min', 'success_rate': '96.8%'},
        'incremental_mode': {'time': '12min', 'success_rate': '98.2%'}
    }
}
```

## æ€»ç»“

XTFé¢å‘ç”µå­è¡¨æ ¼çš„ç®—æ³•è®¾è®¡é€šè¿‡æ·±å…¥åˆ†æé£ä¹¦APIç‰¹æ€§ï¼Œå®ç°äº†å››ç§åŒæ­¥æ¨¡å¼å’Œä¸‰å±‚ä¿éšœæœºåˆ¶çš„å®Œç¾ç»“åˆï¼š

### æ ¸å¿ƒåˆ›æ–°ç‚¹

1. **ç²¾å‡†çš„APIæ¥å£é€‰æ‹©ç­–ç•¥**: æ¯ç§åŒæ­¥æ¨¡å¼éƒ½é€‰æ‹©æœ€é€‚åˆçš„APIæ¥å£ç»„åˆï¼Œå……åˆ†å‘æŒ¥å„æ¥å£ç‰¹æ€§
2. **ä¸‰å±‚é€’è¿›å¼ä¿éšœæœºåˆ¶**: ä»é¢„åˆ†å—åˆ°è‡ªåŠ¨äºŒåˆ†å†åˆ°ç½‘ç»œé‡è¯•ï¼Œç¡®ä¿ä»»æ„è§„æ¨¡æ•°æ®çš„ç¨³å®šå¤„ç†  
3. **æ™ºèƒ½å‚æ•°ä¼˜åŒ–ç®—æ³•**: åŸºäºæ•°æ®ç‰¹å¾å’Œå†å²æˆåŠŸç‡åŠ¨æ€è°ƒä¼˜ï¼Œå¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§
4. **å®Œå–„çš„é”™è¯¯æ¢å¤ä½“ç³»**: åŒºåˆ†é”™è¯¯ç±»å‹ï¼Œé‡‡ç”¨é’ˆå¯¹æ€§çš„æ¢å¤ç­–ç•¥

### å®é™…åº”ç”¨ä»·å€¼

- **ç¨³å®šæ€§**: é€šè¿‡ä¸‰å±‚ä¿éšœæœºåˆ¶ï¼Œå®ç°99%+çš„æˆåŠŸç‡
- **æ•ˆç‡**: æ™ºèƒ½åˆ†å—å’Œå‚æ•°ä¼˜åŒ–ï¼Œæ¯”ä¼ ç»Ÿæ–¹æ¡ˆæå‡30-50%æ•ˆç‡  
- **å¯æ‰©å±•æ€§**: æ¨¡å—åŒ–è®¾è®¡æ”¯æŒçµæ´»æ‰©å±•å’Œå®šåˆ¶
- **ç”¨æˆ·ä½“éªŒ**: ä¸°å¯Œçš„æ—¥å¿—å’Œç›‘æ§ï¼Œæä¾›é€æ˜çš„å¤„ç†è¿‡ç¨‹

è¿™å¥—ç®—æ³•è®¾è®¡ä¸ä»…è§£å†³äº†å½“å‰å¤§æ•°æ®åŒæ­¥çš„æŠ€æœ¯æŒ‘æˆ˜ï¼Œæ›´ä¸ºæœªæ¥çš„åŠŸèƒ½æ‰©å±•å’Œæ€§èƒ½ä¼˜åŒ–å¥ å®šäº†åšå®åŸºç¡€ã€‚é€šè¿‡æŒç»­çš„å®è·µä¼˜åŒ–ï¼ŒXTFå·²æˆä¸ºä¼ä¸šçº§æ•°æ®åŒæ­¥çš„å¯é è§£å†³æ–¹æ¡ˆã€‚